* 
* ==> Audit <==
* |---------|-------------------------|----------|---------|---------|-------------------------------|-------------------------------|
| Command |          Args           | Profile  |  User   | Version |          Start Time           |           End Time            |
|---------|-------------------------|----------|---------|---------|-------------------------------|-------------------------------|
| delete  |                         | minikube | shaheer | v1.25.2 | Tue, 12 Apr 2022 21:28:03 IST | Tue, 12 Apr 2022 21:28:04 IST |
| start   |                         | minikube | shaheer | v1.25.2 | Tue, 12 Apr 2022 23:05:00 IST | Tue, 12 Apr 2022 23:12:23 IST |
| start   |                         | minikube | shaheer | v1.25.2 | Tue, 12 Apr 2022 23:31:18 IST | Tue, 12 Apr 2022 23:31:24 IST |
| cache   | add shaheer/posts:0.0.1 | minikube | shaheer | v1.25.2 | Tue, 12 Apr 2022 23:50:59 IST | Tue, 12 Apr 2022 23:51:07 IST |
| start   |                         | minikube | shaheer | v1.25.2 | Tue, 12 Apr 2022 23:55:51 IST | Tue, 12 Apr 2022 23:56:30 IST |
| start   |                         | minikube | shaheer | v1.25.2 | Thu, 14 Apr 2022 21:36:26 IST | Thu, 14 Apr 2022 21:36:47 IST |
| start   |                         | minikube | shaheer | v1.25.2 | Thu, 14 Apr 2022 21:59:23 IST | Thu, 14 Apr 2022 21:59:44 IST |
| start   |                         | minikube | shaheer | v1.25.2 | Thu, 14 Apr 2022 22:19:50 IST | Thu, 14 Apr 2022 22:20:12 IST |
| start   |                         | minikube | shaheer | v1.25.2 | Thu, 14 Apr 2022 22:41:20 IST | Thu, 14 Apr 2022 22:41:40 IST |
| start   |                         | minikube | shaheer | v1.25.2 | Fri, 15 Apr 2022 00:46:35 IST | Fri, 15 Apr 2022 00:46:56 IST |
| start   |                         | minikube | shaheer | v1.25.2 | Fri, 15 Apr 2022 13:43:20 IST | Fri, 15 Apr 2022 13:43:42 IST |
| ip      |                         | minikube | shaheer | v1.25.2 | Fri, 15 Apr 2022 14:18:02 IST | Fri, 15 Apr 2022 14:18:02 IST |
| start   |                         | minikube | shaheer | v1.25.2 | Fri, 15 Apr 2022 14:35:42 IST | Fri, 15 Apr 2022 14:36:03 IST |
| ip      |                         | minikube | shaheer | v1.25.2 | Fri, 15 Apr 2022 14:38:29 IST | Fri, 15 Apr 2022 14:38:30 IST |
| start   |                         | minikube | shaheer | v1.25.2 | Fri, 15 Apr 2022 22:05:41 IST | Fri, 15 Apr 2022 22:05:45 IST |
| ip      |                         | minikube | shaheer | v1.25.2 | Fri, 15 Apr 2022 22:52:29 IST | Fri, 15 Apr 2022 22:52:29 IST |
| ip      |                         | minikube | shaheer | v1.25.2 | Fri, 15 Apr 2022 23:30:49 IST | Fri, 15 Apr 2022 23:30:49 IST |
| start   |                         | minikube | shaheer | v1.25.2 | Sat, 16 Apr 2022 20:10:14 IST | Sat, 16 Apr 2022 20:10:37 IST |
| ip      |                         | minikube | shaheer | v1.25.2 | Sat, 16 Apr 2022 20:39:36 IST | Sat, 16 Apr 2022 20:39:36 IST |
|---------|-------------------------|----------|---------|---------|-------------------------------|-------------------------------|

* 
* ==> Last Start <==
* Log file created at: 2022/04/16 20:10:14
Running on machine: shaheer-Vostro-3558
Binary: Built with gc go1.17.7 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0416 20:10:14.288040    4485 out.go:297] Setting OutFile to fd 1 ...
I0416 20:10:14.288120    4485 out.go:349] isatty.IsTerminal(1) = true
I0416 20:10:14.288123    4485 out.go:310] Setting ErrFile to fd 2...
I0416 20:10:14.288127    4485 out.go:349] isatty.IsTerminal(2) = true
I0416 20:10:14.288228    4485 root.go:315] Updating PATH: /home/shaheer/.minikube/bin
I0416 20:10:14.289561    4485 out.go:304] Setting JSON to false
I0416 20:10:14.308498    4485 start.go:112] hostinfo: {"hostname":"shaheer-Vostro-3558","uptime":225,"bootTime":1650119790,"procs":242,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"20.04","kernelVersion":"5.13.0-39-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"e956d0c2-4a5d-4415-a869-13bf7f4359e8"}
I0416 20:10:14.308664    4485 start.go:122] virtualization: kvm host
I0416 20:10:14.312752    4485 out.go:176] üòÑ  minikube v1.25.2 on Ubuntu 20.04
I0416 20:10:14.315505    4485 notify.go:193] Checking for updates...
I0416 20:10:14.320152    4485 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.23.3
I0416 20:10:14.322145    4485 driver.go:344] Setting default libvirt URI to qemu:///system
I0416 20:10:14.517102    4485 docker.go:132] docker version: linux-20.10.14
I0416 20:10:14.518708    4485 cli_runner.go:133] Run: docker system info --format "{{json .}}"
I0416 20:10:14.972730    4485 info.go:263] docker info: {ID:VGYA:3TLC:D6NC:INSA:ZRRY:OCJI:HAVG:CDSA:A5KX:T2DY:KMQG:DRV7 Containers:8 ContainersRunning:0 ContainersPaused:0 ContainersStopped:8 Images:72 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:33 SystemTime:2022-04-16 20:10:14.548027593 +0530 IST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.13.0-39-generic OperatingSystem:Ubuntu 20.04.4 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8255537152 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:shaheer-Vostro-3558 Labels:[] ExperimentalBuild:false ServerVersion:20.10.14 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3df54a852345ae127d1fa3092b95168e4a88e2f8 Expected:3df54a852345ae127d1fa3092b95168e4a88e2f8} RuncCommit:{ID:v1.0.3-0-gf46b6ba Expected:v1.0.3-0-gf46b6ba} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=default] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.8.1-docker] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I0416 20:10:14.972881    4485 docker.go:237] overlay module found
I0416 20:10:14.977188    4485 out.go:176] ‚ú®  Using the docker driver based on existing profile
I0416 20:10:14.977212    4485 start.go:281] selected driver: docker
I0416 20:10:14.977216    4485 start.go:798] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.23.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:5m}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/shaheer:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false}
I0416 20:10:14.977320    4485 start.go:809] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0416 20:10:14.977495    4485 cli_runner.go:133] Run: docker system info --format "{{json .}}"
I0416 20:10:15.064598    4485 info.go:263] docker info: {ID:VGYA:3TLC:D6NC:INSA:ZRRY:OCJI:HAVG:CDSA:A5KX:T2DY:KMQG:DRV7 Containers:8 ContainersRunning:0 ContainersPaused:0 ContainersStopped:8 Images:72 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:33 SystemTime:2022-04-16 20:10:15.006044779 +0530 IST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.13.0-39-generic OperatingSystem:Ubuntu 20.04.4 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8255537152 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:shaheer-Vostro-3558 Labels:[] ExperimentalBuild:false ServerVersion:20.10.14 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3df54a852345ae127d1fa3092b95168e4a88e2f8 Expected:3df54a852345ae127d1fa3092b95168e4a88e2f8} RuncCommit:{ID:v1.0.3-0-gf46b6ba Expected:v1.0.3-0-gf46b6ba} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=default] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.8.1-docker] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I0416 20:10:15.065185    4485 cni.go:93] Creating CNI manager for ""
I0416 20:10:15.065192    4485 cni.go:167] CNI unnecessary in this configuration, recommending no CNI
I0416 20:10:15.065198    4485 start_flags.go:302] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.23.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:5m}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/shaheer:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false}
I0416 20:10:15.072007    4485 out.go:176] üëç  Starting control plane node minikube in cluster minikube
I0416 20:10:15.072758    4485 cache.go:120] Beginning downloading kic base image for docker with docker
I0416 20:10:15.076575    4485 out.go:176] üöú  Pulling base image ...
I0416 20:10:15.076617    4485 preload.go:132] Checking if preload exists for k8s version v1.23.3 and runtime docker
I0416 20:10:15.076682    4485 preload.go:148] Found local preload: /home/shaheer/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v17-v1.23.3-docker-overlay2-amd64.tar.lz4
I0416 20:10:15.076688    4485 cache.go:57] Caching tarball of preloaded images
I0416 20:10:15.076688    4485 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 in local docker daemon
I0416 20:10:15.076865    4485 preload.go:174] Found /home/shaheer/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v17-v1.23.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0416 20:10:15.076876    4485 cache.go:60] Finished verifying existence of preloaded tar for  v1.23.3 on docker
I0416 20:10:15.076993    4485 profile.go:148] Saving config to /home/shaheer/.minikube/profiles/minikube/config.json ...
I0416 20:10:15.124499    4485 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 in local docker daemon, skipping pull
I0416 20:10:15.124518    4485 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 exists in daemon, skipping load
I0416 20:10:15.124536    4485 cache.go:208] Successfully downloaded all kic artifacts
I0416 20:10:15.124558    4485 start.go:313] acquiring machines lock for minikube: {Name:mk86261bd6968a807e0d001e26703388113ebb53 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0416 20:10:15.124666    4485 start.go:317] acquired machines lock for "minikube" in 94.252¬µs
I0416 20:10:15.124683    4485 start.go:93] Skipping create...Using existing machine configuration
I0416 20:10:15.124691    4485 fix.go:55] fixHost starting: 
I0416 20:10:15.124889    4485 cli_runner.go:133] Run: docker container inspect minikube --format={{.State.Status}}
I0416 20:10:15.163733    4485 fix.go:108] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0416 20:10:15.163751    4485 fix.go:134] unexpected machine state, will restart: <nil>
I0416 20:10:15.167054    4485 out.go:176] üîÑ  Restarting existing docker container for "minikube" ...
I0416 20:10:15.167106    4485 cli_runner.go:133] Run: docker start minikube
I0416 20:10:15.697424    4485 cli_runner.go:133] Run: docker container inspect minikube --format={{.State.Status}}
I0416 20:10:15.727416    4485 kic.go:420] container "minikube" state is running.
I0416 20:10:15.727775    4485 cli_runner.go:133] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0416 20:10:15.760813    4485 profile.go:148] Saving config to /home/shaheer/.minikube/profiles/minikube/config.json ...
I0416 20:10:15.760981    4485 machine.go:88] provisioning docker machine ...
I0416 20:10:15.760991    4485 ubuntu.go:169] provisioning hostname "minikube"
I0416 20:10:15.761037    4485 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0416 20:10:15.793023    4485 main.go:130] libmachine: Using SSH client type: native
I0416 20:10:15.793384    4485 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x7a12c0] 0x7a43a0 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0416 20:10:15.793392    4485 main.go:130] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0416 20:10:15.793821    4485 main.go:130] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:42114->127.0.0.1:49157: read: connection reset by peer
I0416 20:10:18.955104    4485 main.go:130] libmachine: SSH cmd err, output: <nil>: minikube

I0416 20:10:18.955148    4485 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0416 20:10:18.982280    4485 main.go:130] libmachine: Using SSH client type: native
I0416 20:10:18.982399    4485 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x7a12c0] 0x7a43a0 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0416 20:10:18.982410    4485 main.go:130] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0416 20:10:19.110487    4485 main.go:130] libmachine: SSH cmd err, output: <nil>: 
I0416 20:10:19.110505    4485 ubuntu.go:175] set auth options {CertDir:/home/shaheer/.minikube CaCertPath:/home/shaheer/.minikube/certs/ca.pem CaPrivateKeyPath:/home/shaheer/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/shaheer/.minikube/machines/server.pem ServerKeyPath:/home/shaheer/.minikube/machines/server-key.pem ClientKeyPath:/home/shaheer/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/shaheer/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/shaheer/.minikube}
I0416 20:10:19.110530    4485 ubuntu.go:177] setting up certificates
I0416 20:10:19.110538    4485 provision.go:83] configureAuth start
I0416 20:10:19.110590    4485 cli_runner.go:133] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0416 20:10:19.141313    4485 provision.go:138] copyHostCerts
I0416 20:10:19.142428    4485 exec_runner.go:144] found /home/shaheer/.minikube/ca.pem, removing ...
I0416 20:10:19.142461    4485 exec_runner.go:207] rm: /home/shaheer/.minikube/ca.pem
I0416 20:10:19.142513    4485 exec_runner.go:151] cp: /home/shaheer/.minikube/certs/ca.pem --> /home/shaheer/.minikube/ca.pem (1082 bytes)
I0416 20:10:19.143054    4485 exec_runner.go:144] found /home/shaheer/.minikube/cert.pem, removing ...
I0416 20:10:19.143059    4485 exec_runner.go:207] rm: /home/shaheer/.minikube/cert.pem
I0416 20:10:19.143095    4485 exec_runner.go:151] cp: /home/shaheer/.minikube/certs/cert.pem --> /home/shaheer/.minikube/cert.pem (1123 bytes)
I0416 20:10:19.144392    4485 exec_runner.go:144] found /home/shaheer/.minikube/key.pem, removing ...
I0416 20:10:19.144396    4485 exec_runner.go:207] rm: /home/shaheer/.minikube/key.pem
I0416 20:10:19.144598    4485 exec_runner.go:151] cp: /home/shaheer/.minikube/certs/key.pem --> /home/shaheer/.minikube/key.pem (1679 bytes)
I0416 20:10:19.144779    4485 provision.go:112] generating server cert: /home/shaheer/.minikube/machines/server.pem ca-key=/home/shaheer/.minikube/certs/ca.pem private-key=/home/shaheer/.minikube/certs/ca-key.pem org=shaheer.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0416 20:10:19.287527    4485 provision.go:172] copyRemoteCerts
I0416 20:10:19.287603    4485 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0416 20:10:19.287631    4485 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0416 20:10:19.316129    4485 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/shaheer/.minikube/machines/minikube/id_rsa Username:docker}
I0416 20:10:19.415089    4485 ssh_runner.go:362] scp /home/shaheer/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0416 20:10:19.439652    4485 ssh_runner.go:362] scp /home/shaheer/.minikube/machines/server.pem --> /etc/docker/server.pem (1204 bytes)
I0416 20:10:19.460966    4485 ssh_runner.go:362] scp /home/shaheer/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0416 20:10:19.481064    4485 provision.go:86] duration metric: configureAuth took 370.516834ms
I0416 20:10:19.481078    4485 ubuntu.go:193] setting minikube options for container-runtime
I0416 20:10:19.481292    4485 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.23.3
I0416 20:10:19.481336    4485 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0416 20:10:19.510873    4485 main.go:130] libmachine: Using SSH client type: native
I0416 20:10:19.511017    4485 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x7a12c0] 0x7a43a0 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0416 20:10:19.511024    4485 main.go:130] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0416 20:10:19.644991    4485 main.go:130] libmachine: SSH cmd err, output: <nil>: overlay

I0416 20:10:19.645005    4485 ubuntu.go:71] root file system type: overlay
I0416 20:10:19.645185    4485 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0416 20:10:19.645233    4485 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0416 20:10:19.674154    4485 main.go:130] libmachine: Using SSH client type: native
I0416 20:10:19.674287    4485 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x7a12c0] 0x7a43a0 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0416 20:10:19.674355    4485 main.go:130] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0416 20:10:19.818314    4485 main.go:130] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0416 20:10:19.818367    4485 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0416 20:10:19.849680    4485 main.go:130] libmachine: Using SSH client type: native
I0416 20:10:19.849829    4485 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x7a12c0] 0x7a43a0 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0416 20:10:19.849847    4485 main.go:130] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0416 20:10:19.988108    4485 main.go:130] libmachine: SSH cmd err, output: <nil>: 
I0416 20:10:19.988123    4485 machine.go:91] provisioned docker machine in 4.227136294s
I0416 20:10:19.988129    4485 start.go:267] post-start starting for "minikube" (driver="docker")
I0416 20:10:19.988133    4485 start.go:277] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0416 20:10:19.988186    4485 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0416 20:10:19.988247    4485 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0416 20:10:20.020088    4485 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/shaheer/.minikube/machines/minikube/id_rsa Username:docker}
I0416 20:10:20.115588    4485 ssh_runner.go:195] Run: cat /etc/os-release
I0416 20:10:20.118054    4485 main.go:130] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0416 20:10:20.118066    4485 main.go:130] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0416 20:10:20.118074    4485 main.go:130] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0416 20:10:20.118078    4485 info.go:137] Remote host: Ubuntu 20.04.2 LTS
I0416 20:10:20.118084    4485 filesync.go:126] Scanning /home/shaheer/.minikube/addons for local assets ...
I0416 20:10:20.118378    4485 filesync.go:126] Scanning /home/shaheer/.minikube/files for local assets ...
I0416 20:10:20.118461    4485 start.go:270] post-start completed in 130.326665ms
I0416 20:10:20.118488    4485 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0416 20:10:20.118517    4485 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0416 20:10:20.148708    4485 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/shaheer/.minikube/machines/minikube/id_rsa Username:docker}
I0416 20:10:20.242500    4485 out.go:176] 
W0416 20:10:20.242651    4485 out.go:241] üßØ  Docker is nearly out of disk space, which may cause deployments to fail! (95%!o(MISSING)f capacity)
W0416 20:10:20.242765    4485 out.go:241] üí°  Suggestion: 

    Try one or more of the following to free up space on the device:
    
    1. Run "docker system prune" to remove unused Docker data (optionally with "-a")
    2. Increase the storage allocated to Docker for Desktop by clicking on:
    Docker icon > Preferences > Resources > Disk Image Size
    3. Run "minikube ssh -- docker system prune" if using the Docker container runtime
W0416 20:10:20.242837    4485 out.go:241] üçø  Related issue: https://github.com/kubernetes/minikube/issues/9024
I0416 20:10:20.248612    4485 out.go:176] 
I0416 20:10:20.248630    4485 fix.go:57] fixHost completed within 5.123942358s
I0416 20:10:20.248638    4485 start.go:80] releasing machines lock for "minikube", held for 5.123965483s
I0416 20:10:20.248701    4485 cli_runner.go:133] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0416 20:10:20.280084    4485 ssh_runner.go:195] Run: systemctl --version
I0416 20:10:20.280112    4485 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0416 20:10:20.280135    4485 ssh_runner.go:195] Run: curl -sS -m 2 https://k8s.gcr.io/
I0416 20:10:20.280171    4485 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0416 20:10:20.309114    4485 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/shaheer/.minikube/machines/minikube/id_rsa Username:docker}
I0416 20:10:20.311266    4485 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/shaheer/.minikube/machines/minikube/id_rsa Username:docker}
I0416 20:10:20.832456    4485 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0416 20:10:20.843556    4485 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0416 20:10:20.855869    4485 cruntime.go:272] skipping containerd shutdown because we are bound to it
I0416 20:10:20.855914    4485 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0416 20:10:20.867691    4485 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/dockershim.sock
image-endpoint: unix:///var/run/dockershim.sock
" | sudo tee /etc/crictl.yaml"
I0416 20:10:20.883918    4485 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0416 20:10:20.973696    4485 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0416 20:10:21.059169    4485 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0416 20:10:21.068043    4485 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0416 20:10:21.161392    4485 ssh_runner.go:195] Run: sudo systemctl start docker
I0416 20:10:21.170044    4485 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0416 20:10:21.367871    4485 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0416 20:10:21.409910    4485 out.go:203] üê≥  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
I0416 20:10:21.409981    4485 cli_runner.go:133] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0416 20:10:21.439654    4485 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0416 20:10:21.443095    4485 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0416 20:10:21.456050    4485 out.go:176]     ‚ñ™ kubelet.housekeeping-interval=5m
I0416 20:10:21.456148    4485 preload.go:132] Checking if preload exists for k8s version v1.23.3 and runtime docker
I0416 20:10:21.456193    4485 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0416 20:10:21.496368    4485 docker.go:606] Got preloaded images: -- stdout --
shaheerkp/event-bus:latest
shaheerkp/query:latest
shaheerkp/query:<none>
shaheerkp/query:<none>
shaheerkp/posts:latest
shaheerkp/posts:<none>
shaheerkp/posts:<none>
shaheerkp/posts:<none>
shaheerkp/query:<none>
shaheerkp/moderation:latest
shaheerkp/event-bus:<none>
shaheerkp/comments:latest
shaheerkp/posts:<none>
shaheerkp/query:<none>
shaheerkp/moderation:<none>
shaheerkp/comments:<none>
shaheerkp/event-bus:<none>
shaheerkp/posts:<none>
shaheerkp/event-bus:<none>
shaheerkp/posts:<none>
shaheerkp/posts:<none>
shaheerkp/posts:<none>
shaheerkp/posts:<none>
shaheer/posts:0.0.1
k8s.gcr.io/kube-apiserver:v1.23.3
k8s.gcr.io/kube-proxy:v1.23.3
k8s.gcr.io/kube-scheduler:v1.23.3
k8s.gcr.io/kube-controller-manager:v1.23.3
k8s.gcr.io/etcd:3.5.1-0
k8s.gcr.io/coredns/coredns:v1.8.6
k8s.gcr.io/pause:3.6
kubernetesui/dashboard:v2.3.1
kubernetesui/metrics-scraper:v1.0.7
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0416 20:10:21.496420    4485 docker.go:537] Images already preloaded, skipping extraction
I0416 20:10:21.496465    4485 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0416 20:10:21.532201    4485 docker.go:606] Got preloaded images: -- stdout --
shaheerkp/event-bus:latest
shaheerkp/query:latest
shaheerkp/query:<none>
shaheerkp/query:<none>
shaheerkp/posts:latest
shaheerkp/posts:<none>
shaheerkp/posts:<none>
shaheerkp/posts:<none>
shaheerkp/query:<none>
shaheerkp/moderation:latest
shaheerkp/event-bus:<none>
shaheerkp/comments:latest
shaheerkp/posts:<none>
shaheerkp/query:<none>
shaheerkp/moderation:<none>
shaheerkp/comments:<none>
shaheerkp/event-bus:<none>
shaheerkp/posts:<none>
shaheerkp/event-bus:<none>
shaheerkp/posts:<none>
shaheerkp/posts:<none>
shaheerkp/posts:<none>
shaheerkp/posts:<none>
shaheer/posts:0.0.1
k8s.gcr.io/kube-apiserver:v1.23.3
k8s.gcr.io/kube-proxy:v1.23.3
k8s.gcr.io/kube-scheduler:v1.23.3
k8s.gcr.io/kube-controller-manager:v1.23.3
k8s.gcr.io/etcd:3.5.1-0
k8s.gcr.io/coredns/coredns:v1.8.6
k8s.gcr.io/pause:3.6
kubernetesui/dashboard:v2.3.1
kubernetesui/metrics-scraper:v1.0.7
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0416 20:10:21.532216    4485 cache_images.go:84] Images are preloaded, skipping loading
I0416 20:10:21.532259    4485 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0416 20:10:21.885104    4485 cni.go:93] Creating CNI manager for ""
I0416 20:10:21.885116    4485 cni.go:167] CNI unnecessary in this configuration, recommending no CNI
I0416 20:10:21.885123    4485 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0416 20:10:21.885134    4485 kubeadm.go:158] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.23.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/dockershim.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NoTaintMaster:true NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[]}
I0416 20:10:21.885230    4485 kubeadm.go:162] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.23.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0416 20:10:21.885286    4485 kubeadm.go:936] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.23.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=minikube --housekeeping-interval=5m --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.23.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:5m}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0416 20:10:21.885323    4485 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.23.3
I0416 20:10:21.893923    4485 binaries.go:44] Found k8s binaries, skipping transfer
I0416 20:10:21.893978    4485 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0416 20:10:21.902497    4485 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (361 bytes)
I0416 20:10:21.915716    4485 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0416 20:10:21.931367    4485 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2030 bytes)
I0416 20:10:21.947493    4485 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0416 20:10:21.950861    4485 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0416 20:10:21.959671    4485 certs.go:54] Setting up /home/shaheer/.minikube/profiles/minikube for IP: 192.168.49.2
I0416 20:10:21.959755    4485 certs.go:182] skipping minikubeCA CA generation: /home/shaheer/.minikube/ca.key
I0416 20:10:21.960057    4485 certs.go:182] skipping proxyClientCA CA generation: /home/shaheer/.minikube/proxy-client-ca.key
I0416 20:10:21.960127    4485 certs.go:298] skipping minikube-user signed cert generation: /home/shaheer/.minikube/profiles/minikube/client.key
I0416 20:10:21.960430    4485 certs.go:298] skipping minikube signed cert generation: /home/shaheer/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0416 20:10:21.960551    4485 certs.go:298] skipping aggregator signed cert generation: /home/shaheer/.minikube/profiles/minikube/proxy-client.key
I0416 20:10:21.960643    4485 certs.go:388] found cert: /home/shaheer/.minikube/certs/home/shaheer/.minikube/certs/ca-key.pem (1675 bytes)
I0416 20:10:21.960670    4485 certs.go:388] found cert: /home/shaheer/.minikube/certs/home/shaheer/.minikube/certs/ca.pem (1082 bytes)
I0416 20:10:21.960692    4485 certs.go:388] found cert: /home/shaheer/.minikube/certs/home/shaheer/.minikube/certs/cert.pem (1123 bytes)
I0416 20:10:21.960711    4485 certs.go:388] found cert: /home/shaheer/.minikube/certs/home/shaheer/.minikube/certs/key.pem (1679 bytes)
I0416 20:10:21.961249    4485 ssh_runner.go:362] scp /home/shaheer/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0416 20:10:21.982436    4485 ssh_runner.go:362] scp /home/shaheer/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0416 20:10:22.001780    4485 ssh_runner.go:362] scp /home/shaheer/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0416 20:10:22.023284    4485 ssh_runner.go:362] scp /home/shaheer/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0416 20:10:22.044192    4485 ssh_runner.go:362] scp /home/shaheer/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0416 20:10:22.065460    4485 ssh_runner.go:362] scp /home/shaheer/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0416 20:10:22.088482    4485 ssh_runner.go:362] scp /home/shaheer/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0416 20:10:22.107492    4485 ssh_runner.go:362] scp /home/shaheer/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0416 20:10:22.129921    4485 ssh_runner.go:362] scp /home/shaheer/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0416 20:10:22.152684    4485 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0416 20:10:22.168767    4485 ssh_runner.go:195] Run: openssl version
I0416 20:10:22.176607    4485 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0416 20:10:22.185466    4485 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0416 20:10:22.189114    4485 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Apr 12 17:42 /usr/share/ca-certificates/minikubeCA.pem
I0416 20:10:22.189151    4485 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0416 20:10:22.194924    4485 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0416 20:10:22.203645    4485 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.23.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:5m}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/shaheer:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false}
I0416 20:10:22.203754    4485 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0416 20:10:22.232893    4485 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0416 20:10:22.239860    4485 kubeadm.go:402] found existing configuration files, will attempt cluster restart
I0416 20:10:22.239870    4485 kubeadm.go:601] restartCluster start
I0416 20:10:22.239907    4485 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0416 20:10:22.247327    4485 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0416 20:10:22.247788    4485 kubeconfig.go:92] found "minikube" server: "https://192.168.49.2:8443"
I0416 20:10:22.260056    4485 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0416 20:10:22.268185    4485 api_server.go:165] Checking apiserver status ...
I0416 20:10:22.268225    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0416 20:10:22.283788    4485 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0416 20:10:22.484274    4485 api_server.go:165] Checking apiserver status ...
I0416 20:10:22.484630    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0416 20:10:22.504193    4485 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0416 20:10:22.684498    4485 api_server.go:165] Checking apiserver status ...
I0416 20:10:22.684576    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0416 20:10:22.705414    4485 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0416 20:10:22.884810    4485 api_server.go:165] Checking apiserver status ...
I0416 20:10:22.884921    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0416 20:10:22.905102    4485 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0416 20:10:23.084425    4485 api_server.go:165] Checking apiserver status ...
I0416 20:10:23.084580    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0416 20:10:23.105597    4485 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0416 20:10:23.284913    4485 api_server.go:165] Checking apiserver status ...
I0416 20:10:23.285017    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0416 20:10:23.305357    4485 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0416 20:10:23.484673    4485 api_server.go:165] Checking apiserver status ...
I0416 20:10:23.484765    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0416 20:10:23.504547    4485 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0416 20:10:23.684933    4485 api_server.go:165] Checking apiserver status ...
I0416 20:10:23.685063    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0416 20:10:23.704586    4485 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0416 20:10:23.884890    4485 api_server.go:165] Checking apiserver status ...
I0416 20:10:23.884982    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0416 20:10:23.905512    4485 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0416 20:10:24.083865    4485 api_server.go:165] Checking apiserver status ...
I0416 20:10:24.083979    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0416 20:10:24.103304    4485 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0416 20:10:24.284667    4485 api_server.go:165] Checking apiserver status ...
I0416 20:10:24.284781    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0416 20:10:24.305648    4485 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0416 20:10:24.484880    4485 api_server.go:165] Checking apiserver status ...
I0416 20:10:24.484974    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0416 20:10:24.503569    4485 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0416 20:10:24.684962    4485 api_server.go:165] Checking apiserver status ...
I0416 20:10:24.685090    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0416 20:10:24.704832    4485 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0416 20:10:24.884097    4485 api_server.go:165] Checking apiserver status ...
I0416 20:10:24.884191    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0416 20:10:24.903050    4485 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0416 20:10:25.084460    4485 api_server.go:165] Checking apiserver status ...
I0416 20:10:25.084594    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0416 20:10:25.103842    4485 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0416 20:10:25.284065    4485 api_server.go:165] Checking apiserver status ...
I0416 20:10:25.284179    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0416 20:10:25.302374    4485 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0416 20:10:25.302382    4485 api_server.go:165] Checking apiserver status ...
I0416 20:10:25.302422    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0416 20:10:25.314808    4485 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0416 20:10:25.314820    4485 kubeadm.go:576] needs reconfigure: apiserver error: timed out waiting for the condition
I0416 20:10:25.314825    4485 kubeadm.go:1067] stopping kube-system containers ...
I0416 20:10:25.314864    4485 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0416 20:10:25.348362    4485 docker.go:438] Stopping containers: [4c55131341a1 cfb0ecf2c623 d970c9e543ec 213fcfdf45d0 9ce510f6dcda 9cfd7ba83291 4480a757c9f5 b5855d5d8359 1f581085ed20 a3d411c9ef50 3e74d0505e44 e8f6ef6e226e e6ed0736e14e 6d10eb0a6a15 6aca466eaf7d 906a5499c401 518547874abe 8e8aaba871ed 5ecae4996f36 e98c4cc27858 cbd38fc3b4e9 000f5476359b 097af5284671 e8c543ea4fde 1b20eba40682 7767b46dd533 80dce0e7f73b]
I0416 20:10:25.348418    4485 ssh_runner.go:195] Run: docker stop 4c55131341a1 cfb0ecf2c623 d970c9e543ec 213fcfdf45d0 9ce510f6dcda 9cfd7ba83291 4480a757c9f5 b5855d5d8359 1f581085ed20 a3d411c9ef50 3e74d0505e44 e8f6ef6e226e e6ed0736e14e 6d10eb0a6a15 6aca466eaf7d 906a5499c401 518547874abe 8e8aaba871ed 5ecae4996f36 e98c4cc27858 cbd38fc3b4e9 000f5476359b 097af5284671 e8c543ea4fde 1b20eba40682 7767b46dd533 80dce0e7f73b
I0416 20:10:25.381005    4485 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0416 20:10:25.390189    4485 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0416 20:10:25.397200    4485 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Apr 12 17:42 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Apr 15 09:05 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Apr 12 17:42 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Apr 15 09:05 /etc/kubernetes/scheduler.conf

I0416 20:10:25.397252    4485 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0416 20:10:25.405945    4485 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0416 20:10:25.412924    4485 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0416 20:10:25.420437    4485 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0416 20:10:25.420475    4485 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0416 20:10:25.429802    4485 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0416 20:10:25.435975    4485 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0416 20:10:25.436006    4485 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0416 20:10:25.442696    4485 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0416 20:10:25.451223    4485 kubeadm.go:678] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0416 20:10:25.451240    4485 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0416 20:10:25.616110    4485 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0416 20:10:26.645582    4485 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.02945533s)
I0416 20:10:26.645598    4485 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0416 20:10:26.791013    4485 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0416 20:10:26.833776    4485 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0416 20:10:26.872528    4485 api_server.go:51] waiting for apiserver process to appear ...
I0416 20:10:26.872561    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0416 20:10:27.387572    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0416 20:10:27.887609    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0416 20:10:28.386992    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0416 20:10:28.887295    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0416 20:10:29.387930    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0416 20:10:29.403106    4485 api_server.go:71] duration metric: took 2.530576913s to wait for apiserver process to appear ...
I0416 20:10:29.403117    4485 api_server.go:87] waiting for apiserver healthz status ...
I0416 20:10:29.403125    4485 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0416 20:10:29.403362    4485 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0416 20:10:29.904013    4485 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0416 20:10:29.904270    4485 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0416 20:10:30.403569    4485 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0416 20:10:30.403917    4485 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0416 20:10:30.903494    4485 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0416 20:10:30.903786    4485 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0416 20:10:31.404005    4485 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0416 20:10:33.288883    4485 api_server.go:266] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0416 20:10:33.288896    4485 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0416 20:10:33.404120    4485 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0416 20:10:33.408709    4485 api_server.go:266] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
W0416 20:10:33.408727    4485 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
I0416 20:10:33.904121    4485 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0416 20:10:33.911609    4485 api_server.go:266] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
W0416 20:10:33.911623    4485 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
I0416 20:10:34.404263    4485 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0416 20:10:34.408881    4485 api_server.go:266] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
W0416 20:10:34.408902    4485 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
I0416 20:10:34.904060    4485 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0416 20:10:34.911375    4485 api_server.go:266] https://192.168.49.2:8443/healthz returned 200:
ok
I0416 20:10:34.922721    4485 api_server.go:140] control plane version: v1.23.3
I0416 20:10:34.922734    4485 api_server.go:130] duration metric: took 5.519613367s to wait for apiserver health ...
I0416 20:10:34.922741    4485 cni.go:93] Creating CNI manager for ""
I0416 20:10:34.922746    4485 cni.go:167] CNI unnecessary in this configuration, recommending no CNI
I0416 20:10:34.922752    4485 system_pods.go:43] waiting for kube-system pods to appear ...
I0416 20:10:34.929719    4485 system_pods.go:59] 7 kube-system pods found
I0416 20:10:34.929735    4485 system_pods.go:61] "coredns-64897985d-fbvmc" [8ef4f38c-7fff-45b5-91e0-700ae7703b6e] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0416 20:10:34.929739    4485 system_pods.go:61] "etcd-minikube" [8b55956b-a195-476b-9bfa-248743d405f7] Running
I0416 20:10:34.929742    4485 system_pods.go:61] "kube-apiserver-minikube" [8680fe41-1b88-4c1f-8689-e1fc9b771b71] Running
I0416 20:10:34.929745    4485 system_pods.go:61] "kube-controller-manager-minikube" [b593a0a2-c950-46bf-8461-13e5c58fdd0f] Running
I0416 20:10:34.929748    4485 system_pods.go:61] "kube-proxy-mvr7q" [c84ed7db-dd97-4339-89a4-a8bbbeee6751] Running
I0416 20:10:34.929751    4485 system_pods.go:61] "kube-scheduler-minikube" [ae109bae-fcf0-4212-9b9b-a349aeaa439a] Running
I0416 20:10:34.929754    4485 system_pods.go:61] "storage-provisioner" [eb477805-da5e-4601-8b6a-0f628d8e939e] Running
I0416 20:10:34.929757    4485 system_pods.go:74] duration metric: took 7.001915ms to wait for pod list to return data ...
I0416 20:10:34.929768    4485 node_conditions.go:102] verifying NodePressure condition ...
I0416 20:10:34.932053    4485 node_conditions.go:122] node storage ephemeral capacity is 114854020Ki
I0416 20:10:34.932063    4485 node_conditions.go:123] node cpu capacity is 4
I0416 20:10:34.932071    4485 node_conditions.go:105] duration metric: took 2.300202ms to run NodePressure ...
I0416 20:10:34.932082    4485 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0416 20:10:35.102770    4485 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0416 20:10:35.118720    4485 ops.go:34] apiserver oom_adj: -16
I0416 20:10:35.118732    4485 kubeadm.go:605] restartCluster took 12.878857547s
I0416 20:10:35.118742    4485 kubeadm.go:393] StartCluster complete in 12.915098215s
I0416 20:10:35.118755    4485 settings.go:142] acquiring lock: {Name:mk29605169a3d8ceee564dde415fb1538b987f10 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0416 20:10:35.118872    4485 settings.go:150] Updating kubeconfig:  /home/shaheer/.kube/config
I0416 20:10:35.119775    4485 lock.go:35] WriteFile acquiring /home/shaheer/.kube/config: {Name:mk7b687a9f0d4bf86f54deddb4ea681ad3dc2c61 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0416 20:10:35.122826    4485 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I0416 20:10:35.122862    4485 start.go:208] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0416 20:10:35.122874    4485 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0416 20:10:35.126571    4485 out.go:176] üîé  Verifying Kubernetes components...
I0416 20:10:35.122983    4485 addons.go:415] enableAddons start: toEnable=map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false], additional=[]
I0416 20:10:35.126629    4485 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0416 20:10:35.126650    4485 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I0416 20:10:35.126662    4485 addons.go:153] Setting addon storage-provisioner=true in "minikube"
I0416 20:10:35.123083    4485 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.23.3
W0416 20:10:35.126667    4485 addons.go:165] addon storage-provisioner should already be in state true
I0416 20:10:35.126688    4485 host.go:66] Checking if "minikube" exists ...
I0416 20:10:35.126704    4485 addons.go:65] Setting default-storageclass=true in profile "minikube"
I0416 20:10:35.126714    4485 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0416 20:10:35.126827    4485 cache.go:107] acquiring lock: {Name:mkf23d4228c677e3c936d3f9741e9c406b0240f3 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0416 20:10:35.127353    4485 cli_runner.go:133] Run: docker container inspect minikube --format={{.State.Status}}
I0416 20:10:35.127594    4485 cli_runner.go:133] Run: docker container inspect minikube --format={{.State.Status}}
I0416 20:10:35.127861    4485 cache.go:115] /home/shaheer/.minikube/cache/images/amd64/shaheer/posts_0.0.1 exists
I0416 20:10:35.127881    4485 cache.go:96] cache image "shaheer/posts:0.0.1" -> "/home/shaheer/.minikube/cache/images/amd64/shaheer/posts_0.0.1" took 1.059788ms
I0416 20:10:35.127888    4485 cache.go:80] save to tar file shaheer/posts:0.0.1 -> /home/shaheer/.minikube/cache/images/amd64/shaheer/posts_0.0.1 succeeded
I0416 20:10:35.127898    4485 cache.go:87] Successfully saved all images to host disk.
I0416 20:10:35.128046    4485 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.23.3
I0416 20:10:35.128234    4485 cli_runner.go:133] Run: docker container inspect minikube --format={{.State.Status}}
I0416 20:10:35.184295    4485 out.go:176]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0416 20:10:35.184415    4485 addons.go:348] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0416 20:10:35.184422    4485 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0416 20:10:35.184464    4485 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0416 20:10:35.184906    4485 addons.go:153] Setting addon default-storageclass=true in "minikube"
W0416 20:10:35.184914    4485 addons.go:165] addon default-storageclass should already be in state true
I0416 20:10:35.184932    4485 host.go:66] Checking if "minikube" exists ...
I0416 20:10:35.185238    4485 cli_runner.go:133] Run: docker container inspect minikube --format={{.State.Status}}
I0416 20:10:35.199825    4485 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0416 20:10:35.199858    4485 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0416 20:10:35.224239    4485 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/shaheer/.minikube/machines/minikube/id_rsa Username:docker}
I0416 20:10:35.260282    4485 addons.go:348] installing /etc/kubernetes/addons/storageclass.yaml
I0416 20:10:35.260292    4485 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0416 20:10:35.260328    4485 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0416 20:10:35.267574    4485 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/shaheer/.minikube/machines/minikube/id_rsa Username:docker}
I0416 20:10:35.312652    4485 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/shaheer/.minikube/machines/minikube/id_rsa Username:docker}
I0416 20:10:35.341150    4485 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0416 20:10:35.502843    4485 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0416 20:10:36.488920    4485 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (1.366030984s)
I0416 20:10:36.488973    4485 start.go:757] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0416 20:10:36.488997    4485 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (1.36236151s)
I0416 20:10:36.489019    4485 api_server.go:51] waiting for apiserver process to appear ...
I0416 20:10:36.489045    4485 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0416 20:10:37.057088    4485 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.715901503s)
I0416 20:10:37.057249    4485 ssh_runner.go:235] Completed: docker images --format {{.Repository}}:{{.Tag}}: (1.857397105s)
I0416 20:10:37.057300    4485 docker.go:606] Got preloaded images: -- stdout --
shaheerkp/event-bus:latest
shaheerkp/query:latest
shaheerkp/query:<none>
shaheerkp/query:<none>
shaheerkp/posts:latest
shaheerkp/posts:<none>
shaheerkp/posts:<none>
shaheerkp/posts:<none>
shaheerkp/query:<none>
shaheerkp/moderation:latest
shaheerkp/event-bus:<none>
shaheerkp/comments:latest
shaheerkp/posts:<none>
shaheerkp/query:<none>
shaheerkp/moderation:<none>
shaheerkp/comments:<none>
shaheerkp/event-bus:<none>
shaheerkp/posts:<none>
shaheerkp/event-bus:<none>
shaheerkp/posts:<none>
shaheerkp/posts:<none>
shaheerkp/posts:<none>
shaheerkp/posts:<none>
shaheer/posts:0.0.1
k8s.gcr.io/kube-apiserver:v1.23.3
k8s.gcr.io/kube-controller-manager:v1.23.3
k8s.gcr.io/kube-scheduler:v1.23.3
k8s.gcr.io/kube-proxy:v1.23.3
k8s.gcr.io/etcd:3.5.1-0
k8s.gcr.io/coredns/coredns:v1.8.6
k8s.gcr.io/pause:3.6
kubernetesui/dashboard:v2.3.1
kubernetesui/metrics-scraper:v1.0.7
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0416 20:10:37.057315    4485 cache_images.go:84] Images are preloaded, skipping loading
I0416 20:10:37.057325    4485 cache_images.go:262] succeeded pushing to: minikube
I0416 20:10:37.057332    4485 cache_images.go:263] failed pushing to: 
I0416 20:10:37.057397    4485 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.554536373s)
I0416 20:10:37.067511    4485 out.go:176] üåü  Enabled addons: storage-provisioner, default-storageclass
I0416 20:10:37.067537    4485 addons.go:417] enableAddons completed in 1.944601908s
I0416 20:10:37.057910    4485 api_server.go:71] duration metric: took 1.935024891s to wait for apiserver process to appear ...
I0416 20:10:37.067551    4485 api_server.go:87] waiting for apiserver healthz status ...
I0416 20:10:37.067571    4485 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0416 20:10:37.072194    4485 api_server.go:266] https://192.168.49.2:8443/healthz returned 200:
ok
I0416 20:10:37.072874    4485 api_server.go:140] control plane version: v1.23.3
I0416 20:10:37.072882    4485 api_server.go:130] duration metric: took 5.327806ms to wait for apiserver health ...
I0416 20:10:37.072889    4485 system_pods.go:43] waiting for kube-system pods to appear ...
I0416 20:10:37.077136    4485 system_pods.go:59] 7 kube-system pods found
I0416 20:10:37.077152    4485 system_pods.go:61] "coredns-64897985d-fbvmc" [8ef4f38c-7fff-45b5-91e0-700ae7703b6e] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0416 20:10:37.077155    4485 system_pods.go:61] "etcd-minikube" [8b55956b-a195-476b-9bfa-248743d405f7] Running
I0416 20:10:37.077158    4485 system_pods.go:61] "kube-apiserver-minikube" [8680fe41-1b88-4c1f-8689-e1fc9b771b71] Running
I0416 20:10:37.077162    4485 system_pods.go:61] "kube-controller-manager-minikube" [b593a0a2-c950-46bf-8461-13e5c58fdd0f] Running
I0416 20:10:37.077164    4485 system_pods.go:61] "kube-proxy-mvr7q" [c84ed7db-dd97-4339-89a4-a8bbbeee6751] Running
I0416 20:10:37.077167    4485 system_pods.go:61] "kube-scheduler-minikube" [ae109bae-fcf0-4212-9b9b-a349aeaa439a] Running
I0416 20:10:37.077169    4485 system_pods.go:61] "storage-provisioner" [eb477805-da5e-4601-8b6a-0f628d8e939e] Running
I0416 20:10:37.077173    4485 system_pods.go:74] duration metric: took 4.280455ms to wait for pod list to return data ...
I0416 20:10:37.077178    4485 kubeadm.go:548] duration metric: took 1.954300579s to wait for : map[apiserver:true system_pods:true] ...
I0416 20:10:37.077187    4485 node_conditions.go:102] verifying NodePressure condition ...
I0416 20:10:37.079779    4485 node_conditions.go:122] node storage ephemeral capacity is 114854020Ki
I0416 20:10:37.079790    4485 node_conditions.go:123] node cpu capacity is 4
I0416 20:10:37.079798    4485 node_conditions.go:105] duration metric: took 2.608602ms to run NodePressure ...
I0416 20:10:37.079805    4485 start.go:213] waiting for startup goroutines ...
I0416 20:10:37.135325    4485 start.go:496] kubectl: 1.23.5, cluster: 1.23.3 (minor skew: 0)
I0416 20:10:37.148567    4485 out.go:176] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Sat 2022-04-16 14:40:16 UTC, end at Sat 2022-04-16 15:56:35 UTC. --
Apr 16 14:40:16 minikube systemd[1]: Starting Docker Application Container Engine...
Apr 16 14:40:16 minikube dockerd[206]: time="2022-04-16T14:40:16.661701607Z" level=info msg="Starting up"
Apr 16 14:40:16 minikube dockerd[206]: time="2022-04-16T14:40:16.668886648Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Apr 16 14:40:16 minikube dockerd[206]: time="2022-04-16T14:40:16.668921214Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Apr 16 14:40:16 minikube dockerd[206]: time="2022-04-16T14:40:16.668952690Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Apr 16 14:40:16 minikube dockerd[206]: time="2022-04-16T14:40:16.668965239Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Apr 16 14:40:16 minikube dockerd[206]: time="2022-04-16T14:40:16.674801874Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Apr 16 14:40:16 minikube dockerd[206]: time="2022-04-16T14:40:16.674825484Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Apr 16 14:40:16 minikube dockerd[206]: time="2022-04-16T14:40:16.674844269Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Apr 16 14:40:16 minikube dockerd[206]: time="2022-04-16T14:40:16.674862688Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Apr 16 14:40:16 minikube dockerd[206]: time="2022-04-16T14:40:16.694460212Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Apr 16 14:40:16 minikube dockerd[206]: time="2022-04-16T14:40:16.811008488Z" level=warning msg="Your kernel does not support CPU realtime scheduler"
Apr 16 14:40:16 minikube dockerd[206]: time="2022-04-16T14:40:16.811033055Z" level=warning msg="Your kernel does not support cgroup blkio weight"
Apr 16 14:40:16 minikube dockerd[206]: time="2022-04-16T14:40:16.811039893Z" level=warning msg="Your kernel does not support cgroup blkio weight_device"
Apr 16 14:40:16 minikube dockerd[206]: time="2022-04-16T14:40:16.811488536Z" level=info msg="Loading containers: start."
Apr 16 14:40:17 minikube dockerd[206]: time="2022-04-16T14:40:17.015377821Z" level=info msg="Fixing inconsistent endpoint_cnt for network host. Expected=0, Actual=1"
Apr 16 14:40:17 minikube dockerd[206]: time="2022-04-16T14:40:17.061488365Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Apr 16 14:40:17 minikube dockerd[206]: time="2022-04-16T14:40:17.122792793Z" level=info msg="Loading containers: done."
Apr 16 14:40:17 minikube dockerd[206]: time="2022-04-16T14:40:17.178385091Z" level=info msg="Docker daemon" commit=459d0df graphdriver(s)=overlay2 version=20.10.12
Apr 16 14:40:17 minikube dockerd[206]: time="2022-04-16T14:40:17.179108314Z" level=info msg="Daemon has completed initialization"
Apr 16 14:40:17 minikube systemd[1]: Started Docker Application Container Engine.
Apr 16 14:40:17 minikube dockerd[206]: time="2022-04-16T14:40:17.205768112Z" level=info msg="API listen on [::]:2376"
Apr 16 14:40:17 minikube dockerd[206]: time="2022-04-16T14:40:17.212614130Z" level=info msg="API listen on /var/run/docker.sock"
Apr 16 14:41:07 minikube dockerd[206]: time="2022-04-16T14:41:07.698870262Z" level=info msg="ignoring event" container=0c4bc1e4b953a36a48bc21306071a4d8b16461b4e8346ad5f9d6ad2546dd96f4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 16 14:48:48 minikube dockerd[206]: time="2022-04-16T14:48:48.309637935Z" level=warning msg="reference for unknown type: " digest="sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660" remote="k8s.gcr.io/ingress-nginx/kube-webhook-certgen@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660"
Apr 16 14:48:56 minikube dockerd[206]: time="2022-04-16T14:48:56.353135274Z" level=info msg="ignoring event" container=f7566937e22133fa535b0758af842bf5f09c83c03b11d23df7c9af3437dcf6df module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 16 14:48:56 minikube dockerd[206]: time="2022-04-16T14:48:56.396042790Z" level=info msg="ignoring event" container=a4ce5e29e64a9cb30b592eb2a9fa3c2bfbd5a43873e08f2e488512ba060a54f0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 16 14:48:56 minikube dockerd[206]: time="2022-04-16T14:48:56.452937826Z" level=info msg="ignoring event" container=39a160031915a3e3bb4d01a372b8536d01057780b0f8dda63495acdf32845621 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 16 14:48:56 minikube dockerd[206]: time="2022-04-16T14:48:56.597469985Z" level=info msg="ignoring event" container=e81cd8889192af1d4a1ffe24c38ecbed97b9312add8961607d09b3834136dccd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 16 14:48:57 minikube dockerd[206]: time="2022-04-16T14:48:57.500631902Z" level=info msg="ignoring event" container=24357d7ad809e7ff5da7f793a83728b7a2c674d86e322db9326fff8d7dff6081 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 16 14:49:03 minikube dockerd[206]: time="2022-04-16T14:49:03.580601659Z" level=warning msg="reference for unknown type: " digest="sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c" remote="k8s.gcr.io/ingress-nginx/controller@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c"
Apr 16 15:11:37 minikube dockerd[206]: time="2022-04-16T15:11:37.896887267Z" level=warning msg="reference for unknown type: " digest="sha256:0bc88eb15f9e7f84e8e56c14fa5735aaa488b840983f87bd79b1054190e660de" remote="k8s.gcr.io/ingress-nginx/controller@sha256:0bc88eb15f9e7f84e8e56c14fa5735aaa488b840983f87bd79b1054190e660de"
Apr 16 15:11:48 minikube dockerd[206]: time="2022-04-16T15:11:48.239481614Z" level=info msg="ignoring event" container=daa7a9cfca0ca5c726fc1c7fd05b5a7b8d195fbbdea97a5e1759a8b3a4944a82 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 16 15:11:48 minikube dockerd[206]: time="2022-04-16T15:11:48.324164133Z" level=info msg="ignoring event" container=ec1214217b98e5a442e17c26834dfb6d6c6d531bd53a1bc0ed93ef6b5e57ad4a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 16 15:28:09 minikube dockerd[206]: time="2022-04-16T15:28:09.056384929Z" level=info msg="Container failed to exit within 1s of signal 15 - using the force" container=957a7313ea6348f063f8b6d56ec79981aa583246081b6ccf17fdefb7c39b55f2
Apr 16 15:28:09 minikube dockerd[206]: time="2022-04-16T15:28:09.175018895Z" level=info msg="ignoring event" container=957a7313ea6348f063f8b6d56ec79981aa583246081b6ccf17fdefb7c39b55f2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 16 15:28:09 minikube dockerd[206]: time="2022-04-16T15:28:09.237820284Z" level=info msg="ignoring event" container=8c56eeda48e84f4ce92b35c1393a50f6180512d9abd63022f64b6aa80e1f37fc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 16 15:29:27 minikube dockerd[206]: time="2022-04-16T15:29:27.193877047Z" level=info msg="ignoring event" container=6711c3c17e7733c76c1b5f316f1f9e322c5a6ccf385f2d9f1418495cdf3a436c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 16 15:29:27 minikube dockerd[206]: time="2022-04-16T15:29:27.270596534Z" level=info msg="ignoring event" container=6b3320e6a309483351230e235b1c6982bbe803796740f12e9743aa31cfc4b1c1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 16 15:31:02 minikube dockerd[206]: time="2022-04-16T15:31:02.229982723Z" level=warning msg="reference for unknown type: " digest="sha256:31f47c1e202b39fadecf822a9b76370bd4baed199a005b3e7d4d1455f4fd3fe2" remote="k8s.gcr.io/ingress-nginx/controller@sha256:31f47c1e202b39fadecf822a9b76370bd4baed199a005b3e7d4d1455f4fd3fe2"
Apr 16 15:31:52 minikube dockerd[206]: time="2022-04-16T15:31:52.587376065Z" level=info msg="Container failed to exit within 1s of signal 15 - using the force" container=edbf21b2ed2c44da184478c59e4c7ca8ec149e760493bc35a9d817a821e6d3df
Apr 16 15:31:52 minikube dockerd[206]: time="2022-04-16T15:31:52.707277608Z" level=info msg="ignoring event" container=edbf21b2ed2c44da184478c59e4c7ca8ec149e760493bc35a9d817a821e6d3df module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 16 15:31:52 minikube dockerd[206]: time="2022-04-16T15:31:52.913894121Z" level=info msg="ignoring event" container=d2279b01cef501d7ad53de66494541e688be6a0c309154022b92940415d24ff5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 16 15:37:18 minikube dockerd[206]: time="2022-04-16T15:37:18.179752257Z" level=info msg="ignoring event" container=5051befa38754d2335f25a9aa20b42ce8ef36ddad5bda0b51e8e830a71badea7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 16 15:37:18 minikube dockerd[206]: time="2022-04-16T15:37:18.291894068Z" level=info msg="ignoring event" container=fa953fc9b0bd41a8efc82e20fe6efcb3b02a57fe7bcc379e346deb5a99d460a3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 16 15:41:40 minikube dockerd[206]: time="2022-04-16T15:41:40.186447025Z" level=info msg="ignoring event" container=71dcc78875173fd00b007e1a4a725510c9bc40262fe0d376818e86fec44571d5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 16 15:41:40 minikube dockerd[206]: time="2022-04-16T15:41:40.263625143Z" level=info msg="ignoring event" container=8e53b4c7398d93e1453c27492e333cd763734301b8093404765af71d748d19a6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                                   CREATED             STATE               NAME                      ATTEMPT             POD ID
3cef039b769bf       2461b2698dcd5                                                                                                           15 minutes ago      Running             controller                0                   2f7d051875a2e
e81cd8889192a       c41e9fcadf5a2                                                                                                           About an hour ago   Exited              patch                     1                   24357d7ad809e
a4ce5e29e64a9       k8s.gcr.io/ingress-nginx/kube-webhook-certgen@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660   About an hour ago   Exited              create                    0                   39a160031915a
bcaaac5df9a01       6e38f40d628db                                                                                                           About an hour ago   Running             storage-provisioner       19                  c118fdd4c5e5c
f9466440cd581       shaheerkp/moderation@sha256:f0913531ba55a59168ffb30061a2e10cf04d235dc5574a375895092687490acc                            About an hour ago   Running             moderation                1                   42d336ef52be1
59a9e77f08276       shaheerkp/event-bus@sha256:01b559c1d8533cf661306b0562125a7c4d41f18cea4a248ab4630c5b5ab30385                             About an hour ago   Running             event-bus                 1                   f7d421a8d3f30
f502c8df27410       shaheerkp/query@sha256:46e734b07d2e28683eb04a0b09e65db4f1c48888dcbd818c4fb84a276e36720e                                 About an hour ago   Running             query                     1                   1ee0d337ab107
8009cd8176fcb       shaheerkp/comments@sha256:bab4e2050f60c8106a2f8736cc6e540ed0753b1f2344dc5e93e944e83581755f                              About an hour ago   Running             comments                  1                   b8c576a31d133
b56c681af8321       shaheerkp/posts@sha256:68a477c2c2ac0e13fa124267fd9d77d807602a643696c1619a7a2107bd9d91d7                                 About an hour ago   Running             posts                     1                   267e4bbac1c0a
786d4b281bf7e       9b7cc99821098                                                                                                           About an hour ago   Running             kube-proxy                9                   be56cd68c3cda
0c4bc1e4b953a       6e38f40d628db                                                                                                           About an hour ago   Exited              storage-provisioner       18                  c118fdd4c5e5c
bd1144c858a51       a4ca41631cc7a                                                                                                           About an hour ago   Running             coredns                   9                   d12494a04a204
e48b960b7be69       25f8c7f3da61c                                                                                                           About an hour ago   Running             etcd                      9                   e66484a1c455d
6bb0ccdf221ab       f40be0088a83e                                                                                                           About an hour ago   Running             kube-apiserver            9                   8df457c9220c5
239dee87cc707       b07520cd7ab76                                                                                                           About an hour ago   Running             kube-controller-manager   9                   dfd985efd3804
8ee3b422b61e5       99a3486be4f28                                                                                                           About an hour ago   Running             kube-scheduler            9                   67c9c4a5242ac
853bbad8ec44a       shaheerkp/event-bus@sha256:01b559c1d8533cf661306b0562125a7c4d41f18cea4a248ab4630c5b5ab30385                             20 hours ago        Exited              event-bus                 0                   251f76fb1a5a9
efdbe3acd64d6       shaheerkp/query@sha256:46e734b07d2e28683eb04a0b09e65db4f1c48888dcbd818c4fb84a276e36720e                                 20 hours ago        Exited              query                     0                   fe53d72c291fe
db92d38a0045c       shaheerkp/posts@sha256:68a477c2c2ac0e13fa124267fd9d77d807602a643696c1619a7a2107bd9d91d7                                 20 hours ago        Exited              posts                     0                   31c75e822b218
ec957c1191978       shaheerkp/moderation@sha256:f0913531ba55a59168ffb30061a2e10cf04d235dc5574a375895092687490acc                            20 hours ago        Exited              moderation                0                   f46c23dd355d9
0a68e8511d620       shaheerkp/comments@sha256:bab4e2050f60c8106a2f8736cc6e540ed0753b1f2344dc5e93e944e83581755f                              20 hours ago        Exited              comments                  0                   61de5c60440ee
cfb0ecf2c623a       a4ca41631cc7a                                                                                                           31 hours ago        Exited              coredns                   8                   d970c9e543ec1
9cfd7ba83291a       9b7cc99821098                                                                                                           31 hours ago        Exited              kube-proxy                8                   4480a757c9f50
b5855d5d8359d       25f8c7f3da61c                                                                                                           31 hours ago        Exited              etcd                      8                   6d10eb0a6a159
1f581085ed20e       b07520cd7ab76                                                                                                           31 hours ago        Exited              kube-controller-manager   8                   6aca466eaf7d4
a3d411c9ef509       99a3486be4f28                                                                                                           31 hours ago        Exited              kube-scheduler            8                   e6ed0736e14e9
3e74d0505e442       f40be0088a83e                                                                                                           31 hours ago        Exited              kube-apiserver            8                   e8f6ef6e226e6

* 
* ==> coredns [bd1144c858a5] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration MD5 = cec3c60eb1cc4909fd4579a8d79ea031
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"

* 
* ==> coredns [cfb0ecf2c623] <==
* .:53
[INFO] plugin/reload: Running configuration MD5 = cec3c60eb1cc4909fd4579a8d79ea031
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane,master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=362d5fdc0a3dbee389b3d3f1034e8023e72bd3a7
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2022_04_12T23_12_21_0700
                    minikube.k8s.io/version=v1.25.2
                    node-role.kubernetes.io/control-plane=
                    node-role.kubernetes.io/master=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 12 Apr 2022 17:42:17 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 16 Apr 2022 15:56:28 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 16 Apr 2022 15:52:26 +0000   Tue, 12 Apr 2022 17:42:16 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 16 Apr 2022 15:52:26 +0000   Tue, 12 Apr 2022 17:42:16 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 16 Apr 2022 15:52:26 +0000   Tue, 12 Apr 2022 17:42:16 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 16 Apr 2022 15:52:26 +0000   Tue, 12 Apr 2022 17:42:31 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  114854020Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8062048Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  114854020Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8062048Ki
  pods:               110
System Info:
  Machine ID:                 b6a262faae404a5db719705fd34b5c8b
  System UUID:                b706dfe4-1c97-43a5-bc83-8102b1e1ac16
  Boot ID:                    2eb81bdf-2bcb-4ca8-b4ac-3be622658f55
  Kernel Version:             5.13.0-39-generic
  OS Image:                   Ubuntu 20.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.12
  Kubelet Version:            v1.23.3
  Kube-Proxy Version:         v1.23.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (13 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  default                     comments-depl-7f5ccd866d-xrdk7              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         20h
  default                     event-bus-depl-64dcd86bc5-zfz2w             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         19h
  default                     moderation-depl-75947c4dc5-rm92g            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         20h
  default                     posts-depl-78c8ccdff8-fwlk6                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         20h
  default                     query-depl-86b67cf65f-k64fr                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         19h
  ingress-nginx               ingress-nginx-controller-cc8496874-gqrww    100m (2%!)(MISSING)     0 (0%!)(MISSING)      90Mi (1%!)(MISSING)        0 (0%!)(MISSING)         15m
  kube-system                 coredns-64897985d-fbvmc                     100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     3d22h
  kube-system                 etcd-minikube                               100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         3d22h
  kube-system                 kube-apiserver-minikube                     250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d22h
  kube-system                 kube-controller-manager-minikube            200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d22h
  kube-system                 kube-proxy-mvr7q                            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d22h
  kube-system                 kube-scheduler-minikube                     100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d22h
  kube-system                 storage-provisioner                         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d22h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (21%!)(MISSING)  0 (0%!)(MISSING)
  memory             260Mi (3%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [  +0.000007] ata1.00: irq_stat 0x08000000, interface fatal error
[  +0.000002] ata1: SError: { UnrecovData 10B8B BadCRC }
[  +0.000003] ata1.00: failed command: READ FPDMA QUEUED
[  +0.000001] ata1.00: cmd 60/98:30:e0:ad:a8/00:00:0a:00:00/40 tag 6 ncq dma 77824 in
                       res 40/00:38:c0:d3:9b/00:00:0a:00:00/40 Emask 0x10 (ATA bus error)
[  +0.000005] ata1.00: status: { DRDY }
[  +0.000002] ata1.00: failed command: READ FPDMA QUEUED
[  +0.000001] ata1.00: cmd 60/00:38:c0:d3:9b/01:00:0a:00:00/40 tag 7 ncq dma 131072 in
                       res 40/00:38:c0:d3:9b/00:00:0a:00:00/40 Emask 0x10 (ATA bus error)
[  +0.000004] ata1.00: status: { DRDY }
[  +0.000002] ata1.00: failed command: READ FPDMA QUEUED
[  +0.000001] ata1.00: cmd 60/a8:48:60:70:aa/00:00:0a:00:00/40 tag 9 ncq dma 86016 in
                       res 40/00:38:c0:d3:9b/00:00:0a:00:00/40 Emask 0x10 (ATA bus error)
[  +0.000004] ata1.00: status: { DRDY }
[  +0.000001] ata1.00: failed command: READ FPDMA QUEUED
[  +0.000001] ata1.00: cmd 60/00:98:c8:64:a8/01:00:0c:00:00/40 tag 19 ncq dma 131072 in
                       res 40/00:38:c0:d3:9b/00:00:0a:00:00/40 Emask 0x10 (ATA bus error)
[  +0.000004] ata1.00: status: { DRDY }
[  +0.314888] blk_update_request: I/O error, dev sda, sector 178826720 op 0x0:(READ) flags 0x80700 phys_seg 2 prio class 0
[  +0.000021] blk_update_request: I/O error, dev sda, sector 177984448 op 0x0:(READ) flags 0x80700 phys_seg 3 prio class 0
[  +0.000014] blk_update_request: I/O error, dev sda, sector 178942048 op 0x0:(READ) flags 0x80700 phys_seg 5 prio class 0
[  +0.000014] blk_update_request: I/O error, dev sda, sector 212362440 op 0x0:(READ) flags 0x80700 phys_seg 2 prio class 0
[  +0.201038] ata1.00: exception Emask 0x10 SAct 0xc010100 SErr 0x280100 action 0x6 frozen
[  +0.000016] ata1.00: irq_stat 0x08000000, interface fatal error
[  +0.000002] ata1: SError: { UnrecovData 10B8B BadCRC }
[  +0.000003] ata1.00: failed command: READ FPDMA QUEUED
[  +0.000001] ata1.00: cmd 60/00:40:78:d0:9c/01:00:0a:00:00/40 tag 8 ncq dma 131072 in
                       res 40/00:40:78:d0:9c/00:00:0a:00:00/40 Emask 0x10 (ATA bus error)
[  +0.000005] ata1.00: status: { DRDY }
[  +0.000002] ata1.00: failed command: READ FPDMA QUEUED
[  +0.000001] ata1.00: cmd 60/98:80:b8:a9:a7/00:00:0a:00:00/40 tag 16 ncq dma 77824 in
                       res 40/00:40:78:d0:9c/00:00:0a:00:00/40 Emask 0x10 (ATA bus error)
[  +0.000004] ata1.00: status: { DRDY }
[  +0.000001] ata1.00: failed command: READ FPDMA QUEUED
[  +0.000001] ata1.00: cmd 60/00:d0:b8:0d:38/01:00:0b:00:00/40 tag 26 ncq dma 131072 in
                       res 40/00:40:78:d0:9c/00:00:0a:00:00/40 Emask 0x10 (ATA bus error)
[  +0.000004] ata1.00: status: { DRDY }
[  +0.000001] ata1.00: failed command: READ FPDMA QUEUED
[  +0.000001] ata1.00: cmd 60/00:d8:b8:0e:38/01:00:0b:00:00/40 tag 27 ncq dma 131072 in
                       res 40/00:40:78:d0:9c/00:00:0a:00:00/40 Emask 0x10 (ATA bus error)
[  +0.000003] ata1.00: status: { DRDY }
[  +0.314825] blk_update_request: I/O error, dev sda, sector 178049144 op 0x0:(READ) flags 0x80700 phys_seg 11 prio class 0
[  +0.000022] blk_update_request: I/O error, dev sda, sector 178760120 op 0x0:(READ) flags 0x80700 phys_seg 2 prio class 0
[  +0.000013] blk_update_request: I/O error, dev sda, sector 188222904 op 0x0:(READ) flags 0x80700 phys_seg 2 prio class 0
[  +0.000013] blk_update_request: I/O error, dev sda, sector 188223160 op 0x0:(READ) flags 0x80700 phys_seg 3 prio class 0
[  +7.181261] ata1: limiting SATA link speed to 3.0 Gbps
[  +0.000005] ata1.00: exception Emask 0x10 SAct 0x10 SErr 0x280100 action 0x6 frozen
[  +0.000003] ata1.00: irq_stat 0x08000000, interface fatal error
[  +0.000002] ata1: SError: { UnrecovData 10B8B BadCRC }
[  +0.000003] ata1.00: failed command: READ FPDMA QUEUED
[  +0.000002] ata1.00: cmd 60/00:20:b8:d4:b3/01:00:0a:00:00/40 tag 4 ncq dma 131072 in
                       res 40/00:20:b8:d4:b3/00:00:0a:00:00/40 Emask 0x10 (ATA bus error)
[  +0.000005] ata1.00: status: { DRDY }
[  +0.314951] blk_update_request: I/O error, dev sda, sector 179557560 op 0x0:(READ) flags 0x80700 phys_seg 9 prio class 0
[Apr16 14:41] kauditd_printk_skb: 7 callbacks suppressed
[Apr16 14:44] kauditd_printk_skb: 16 callbacks suppressed
[Apr16 14:53] kauditd_printk_skb: 11 callbacks suppressed
[Apr16 14:57] kauditd_printk_skb: 11 callbacks suppressed
[Apr16 15:05] kauditd_printk_skb: 11 callbacks suppressed
[Apr16 15:38] kauditd_printk_skb: 2 callbacks suppressed

* 
* ==> etcd [b5855d5d8359] <==
* {"level":"info","ts":"2022-04-15T18:41:00.813Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":41292,"took":"587.002¬µs"}
{"level":"info","ts":"2022-04-15T18:46:00.831Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":41502}
{"level":"info","ts":"2022-04-15T18:46:00.832Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":41502,"took":"520.219¬µs"}
{"level":"info","ts":"2022-04-15T18:51:00.851Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":41712}
{"level":"info","ts":"2022-04-15T18:51:00.852Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":41712,"took":"390.913¬µs"}
{"level":"info","ts":"2022-04-15T18:56:00.870Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":41923}
{"level":"info","ts":"2022-04-15T18:56:00.870Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":41923,"took":"399.142¬µs"}
{"level":"info","ts":"2022-04-15T19:01:00.893Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":42133}
{"level":"info","ts":"2022-04-15T19:01:00.893Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":42133,"took":"351.077¬µs"}
{"level":"info","ts":"2022-04-15T19:06:00.900Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":42346}
{"level":"info","ts":"2022-04-15T19:06:00.901Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":42346,"took":"686.971¬µs"}
{"level":"info","ts":"2022-04-15T19:11:00.908Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":42556}
{"level":"info","ts":"2022-04-15T19:11:00.909Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":42556,"took":"587.866¬µs"}
{"level":"info","ts":"2022-04-15T19:16:00.917Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":42850}
{"level":"info","ts":"2022-04-15T19:16:00.918Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":42850,"took":"849.99¬µs"}
{"level":"info","ts":"2022-04-15T19:21:00.926Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":43060}
{"level":"info","ts":"2022-04-15T19:21:00.927Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":43060,"took":"453.681¬µs"}
{"level":"info","ts":"2022-04-15T19:26:00.934Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":43400}
{"level":"info","ts":"2022-04-15T19:26:00.935Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":43400,"took":"612.72¬µs"}
{"level":"info","ts":"2022-04-15T19:31:00.942Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":43678}
{"level":"info","ts":"2022-04-15T19:31:00.943Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":43678,"took":"465.441¬µs"}
{"level":"info","ts":"2022-04-15T19:36:00.963Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":43888}
{"level":"info","ts":"2022-04-15T19:36:00.963Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":43888,"took":"443.886¬µs"}
{"level":"info","ts":"2022-04-15T19:41:00.983Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":44188}
{"level":"info","ts":"2022-04-15T19:41:00.985Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":44188,"took":"1.129223ms"}
{"level":"info","ts":"2022-04-15T19:46:00.999Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":44444}
{"level":"info","ts":"2022-04-15T19:46:01.000Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":44444,"took":"578.059¬µs"}
{"level":"info","ts":"2022-04-15T19:51:01.018Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":44743}
{"level":"info","ts":"2022-04-15T19:51:01.019Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":44743,"took":"581.303¬µs"}
{"level":"info","ts":"2022-04-15T19:52:24.327Z","caller":"traceutil/trace.go:171","msg":"trace[207471272] transaction","detail":"{read_only:false; response_revision:45209; number_of_response:1; }","duration":"219.948417ms","start":"2022-04-15T19:52:24.107Z","end":"2022-04-15T19:52:24.327Z","steps":["trace[207471272] 'process raft request'  (duration: 203.519831ms)","trace[207471272] 'compare'  (duration: 16.361522ms)"],"step_count":2}
{"level":"info","ts":"2022-04-15T19:56:01.038Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":44952}
{"level":"info","ts":"2022-04-15T19:56:01.039Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":44952,"took":"772.877¬µs"}
{"level":"info","ts":"2022-04-15T20:01:01.062Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":45399}
{"level":"info","ts":"2022-04-15T20:01:01.063Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":45399,"took":"811.127¬µs"}
{"level":"info","ts":"2022-04-15T20:06:01.072Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":45647}
{"level":"info","ts":"2022-04-15T20:06:01.073Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":45647,"took":"689.985¬µs"}
{"level":"info","ts":"2022-04-15T20:11:01.082Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":45895}
{"level":"info","ts":"2022-04-15T20:11:01.083Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":45895,"took":"824.768¬µs"}
{"level":"info","ts":"2022-04-15T20:16:01.089Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":46144}
{"level":"info","ts":"2022-04-15T20:16:01.090Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":46144,"took":"625.56¬µs"}
{"level":"info","ts":"2022-04-15T20:21:01.097Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":46356}
{"level":"info","ts":"2022-04-15T20:21:01.098Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":46356,"took":"671.692¬µs"}
{"level":"info","ts":"2022-04-15T20:26:01.106Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":46565}
{"level":"info","ts":"2022-04-15T20:26:01.107Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":46565,"took":"719.675¬µs"}
{"level":"info","ts":"2022-04-15T20:31:01.114Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":46778}
{"level":"info","ts":"2022-04-15T20:31:01.115Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":46778,"took":"705.127¬µs"}
{"level":"info","ts":"2022-04-15T20:32:39.477Z","caller":"etcdserver/server.go:1368","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":60006,"local-member-snapshot-index":50005,"local-member-snapshot-count":10000}
{"level":"info","ts":"2022-04-15T20:32:39.487Z","caller":"etcdserver/server.go:2363","msg":"saved snapshot","snapshot-index":60006}
{"level":"info","ts":"2022-04-15T20:32:39.488Z","caller":"etcdserver/server.go:2393","msg":"compacted Raft logs","compact-index":55006}
{"level":"info","ts":"2022-04-15T20:32:58.660Z","caller":"fileutil/purge.go:77","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/0000000000000007-0000000000002711.snap"}
{"level":"info","ts":"2022-04-15T20:36:01.128Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":46988}
{"level":"info","ts":"2022-04-15T20:36:01.128Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":46988,"took":"441.987¬µs"}
{"level":"info","ts":"2022-04-15T20:37:33.320Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2022-04-15T20:37:33.320Z","caller":"embed/etcd.go:367","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
WARNING: 2022/04/15 20:37:33 [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1:2379 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
WARNING: 2022/04/15 20:37:33 [core] grpc: addrConn.createTransport failed to connect to {192.168.49.2:2379 192.168.49.2:2379 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 192.168.49.2:2379: connect: connection refused". Reconnecting...
{"level":"info","ts":"2022-04-15T20:37:33.381Z","caller":"etcdserver/server.go:1438","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2022-04-15T20:37:33.397Z","caller":"embed/etcd.go:562","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-04-15T20:37:33.401Z","caller":"embed/etcd.go:567","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-04-15T20:37:33.401Z","caller":"embed/etcd.go:369","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [e48b960b7be6] <==
* {"level":"info","ts":"2022-04-16T14:40:30.870Z","caller":"embed/etcd.go:552","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-04-16T14:40:30.870Z","caller":"embed/etcd.go:276","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2022-04-16T14:40:30.870Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2022-04-16T14:40:31.315Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 10"}
{"level":"info","ts":"2022-04-16T14:40:31.315Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 10"}
{"level":"info","ts":"2022-04-16T14:40:31.315Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 10"}
{"level":"info","ts":"2022-04-16T14:40:31.315Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 11"}
{"level":"info","ts":"2022-04-16T14:40:31.315Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 11"}
{"level":"info","ts":"2022-04-16T14:40:31.315Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 11"}
{"level":"info","ts":"2022-04-16T14:40:31.315Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 11"}
{"level":"info","ts":"2022-04-16T14:40:31.319Z","caller":"etcdserver/server.go:2027","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2022-04-16T14:40:31.319Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-04-16T14:40:31.319Z","caller":"etcdmain/main.go:47","msg":"notifying init daemon"}
{"level":"info","ts":"2022-04-16T14:40:31.320Z","caller":"etcdmain/main.go:53","msg":"successfully notified init daemon"}
{"level":"info","ts":"2022-04-16T14:40:31.319Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-04-16T14:40:31.324Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2022-04-16T14:40:31.324Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"warn","ts":"2022-04-16T14:40:37.529Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"331.563679ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128012383248080881 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/kube-system/etcd-minikube.16e6676c59ec33f5\" mod_revision:0 > success:<request_put:<key:\"/registry/events/kube-system/etcd-minikube.16e6676c59ec33f5\" value_size:580 lease:8128012383248080512 >> failure:<>>","response":"size:18"}
{"level":"info","ts":"2022-04-16T14:40:37.529Z","caller":"traceutil/trace.go:171","msg":"trace[1260730531] linearizableReadLoop","detail":"{readStateIndex:60327; appliedIndex:60326; }","duration":"244.205057ms","start":"2022-04-16T14:40:37.285Z","end":"2022-04-16T14:40:37.529Z","steps":["trace[1260730531] 'read index received'  (duration: 239.706366ms)","trace[1260730531] 'applied index is now lower than readState.Index'  (duration: 4.497837ms)"],"step_count":2}
{"level":"info","ts":"2022-04-16T14:40:37.529Z","caller":"traceutil/trace.go:171","msg":"trace[731536713] transaction","detail":"{read_only:false; response_revision:47306; number_of_response:1; }","duration":"332.166284ms","start":"2022-04-16T14:40:37.197Z","end":"2022-04-16T14:40:37.529Z","steps":["trace[731536713] 'compare'  (duration: 331.4836ms)"],"step_count":1}
{"level":"warn","ts":"2022-04-16T14:40:37.529Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"244.315665ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/moderation-depl-75947c4dc5-rm92g\" ","response":"range_response_count:1 size:2703"}
{"level":"info","ts":"2022-04-16T14:40:37.529Z","caller":"traceutil/trace.go:171","msg":"trace[1564119286] range","detail":"{range_begin:/registry/pods/default/moderation-depl-75947c4dc5-rm92g; range_end:; response_count:1; response_revision:47306; }","duration":"244.387238ms","start":"2022-04-16T14:40:37.285Z","end":"2022-04-16T14:40:37.529Z","steps":["trace[1564119286] 'agreement among raft nodes before linearized reading'  (duration: 244.261206ms)"],"step_count":1}
{"level":"warn","ts":"2022-04-16T14:40:37.529Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-04-16T14:40:37.197Z","time spent":"332.262898ms","remote":"127.0.0.1:37840","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":657,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/events/kube-system/etcd-minikube.16e6676c59ec33f5\" mod_revision:0 > success:<request_put:<key:\"/registry/events/kube-system/etcd-minikube.16e6676c59ec33f5\" value_size:580 lease:8128012383248080512 >> failure:<>"}
{"level":"info","ts":"2022-04-16T14:49:37.260Z","caller":"traceutil/trace.go:171","msg":"trace[1787933118] linearizableReadLoop","detail":"{readStateIndex:61015; appliedIndex:61015; }","duration":"446.909774ms","start":"2022-04-16T14:49:36.813Z","end":"2022-04-16T14:49:37.260Z","steps":["trace[1787933118] 'read index received'  (duration: 446.897464ms)","trace[1787933118] 'applied index is now lower than readState.Index'  (duration: 11.728¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-04-16T14:49:37.260Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"446.999943ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-04-16T14:49:37.260Z","caller":"traceutil/trace.go:171","msg":"trace[727320080] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:47881; }","duration":"447.031805ms","start":"2022-04-16T14:49:36.813Z","end":"2022-04-16T14:49:37.260Z","steps":["trace[727320080] 'agreement among raft nodes before linearized reading'  (duration: 446.98477ms)"],"step_count":1}
{"level":"warn","ts":"2022-04-16T14:49:37.260Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-04-16T14:49:36.813Z","time spent":"447.060651ms","remote":"127.0.0.1:37964","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-04-16T14:49:39.329Z","caller":"etcdserver/v3_server.go:815","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128012383248083551,"retry-timeout":"500ms"}
{"level":"info","ts":"2022-04-16T14:49:39.483Z","caller":"traceutil/trace.go:171","msg":"trace[1437860207] linearizableReadLoop","detail":"{readStateIndex:61016; appliedIndex:61016; }","duration":"654.332516ms","start":"2022-04-16T14:49:38.829Z","end":"2022-04-16T14:49:39.483Z","steps":["trace[1437860207] 'read index received'  (duration: 654.328134ms)","trace[1437860207] 'applied index is now lower than readState.Index'  (duration: 3.905¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-04-16T14:49:39.605Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"776.636984ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-04-16T14:49:39.605Z","caller":"traceutil/trace.go:171","msg":"trace[387366134] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:47882; }","duration":"776.695549ms","start":"2022-04-16T14:49:38.829Z","end":"2022-04-16T14:49:39.605Z","steps":["trace[387366134] 'agreement among raft nodes before linearized reading'  (duration: 654.389752ms)","trace[387366134] 'range keys from in-memory index tree'  (duration: 122.236159ms)"],"step_count":2}
{"level":"info","ts":"2022-04-16T14:50:31.429Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":47610}
{"level":"info","ts":"2022-04-16T14:50:31.457Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":47610,"took":"28.522828ms"}
{"level":"info","ts":"2022-04-16T14:55:31.443Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":47943}
{"level":"info","ts":"2022-04-16T14:55:31.444Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":47943,"took":"708.975¬µs"}
{"level":"info","ts":"2022-04-16T15:00:31.450Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":48193}
{"level":"info","ts":"2022-04-16T15:00:31.451Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":48193,"took":"528.418¬µs"}
{"level":"info","ts":"2022-04-16T15:05:31.470Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":48443}
{"level":"info","ts":"2022-04-16T15:05:31.471Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":48443,"took":"488.751¬µs"}
{"level":"info","ts":"2022-04-16T15:10:31.477Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":48693}
{"level":"info","ts":"2022-04-16T15:10:31.479Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":48693,"took":"778.494¬µs"}
{"level":"info","ts":"2022-04-16T15:15:31.520Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":48943}
{"level":"info","ts":"2022-04-16T15:15:31.521Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":48943,"took":"479.62¬µs"}
{"level":"info","ts":"2022-04-16T15:20:31.539Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":49267}
{"level":"info","ts":"2022-04-16T15:20:31.540Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":49267,"took":"629.36¬µs"}
{"level":"info","ts":"2022-04-16T15:25:31.559Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":49517}
{"level":"info","ts":"2022-04-16T15:25:31.560Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":49517,"took":"488.41¬µs"}
{"level":"info","ts":"2022-04-16T15:30:31.567Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":49768}
{"level":"info","ts":"2022-04-16T15:30:31.568Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":49768,"took":"832.313¬µs"}
{"level":"info","ts":"2022-04-16T15:35:31.576Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":50155}
{"level":"info","ts":"2022-04-16T15:35:31.593Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":50155,"took":"17.032684ms"}
{"level":"info","ts":"2022-04-16T15:40:31.585Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":50469}
{"level":"info","ts":"2022-04-16T15:40:31.586Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":50469,"took":"533.94¬µs"}
{"level":"info","ts":"2022-04-16T15:45:31.602Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":50791}
{"level":"info","ts":"2022-04-16T15:45:31.603Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":50791,"took":"547.258¬µs"}
{"level":"info","ts":"2022-04-16T15:50:31.622Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":51134}
{"level":"info","ts":"2022-04-16T15:50:31.640Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":51134,"took":"17.427184ms"}
{"level":"info","ts":"2022-04-16T15:52:26.657Z","caller":"wal/wal.go:782","msg":"created a new WAL segment","path":"/var/lib/minikube/etcd/member/wal/0000000000000001-000000000000ff73.wal"}
{"level":"info","ts":"2022-04-16T15:55:31.643Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":51387}
{"level":"info","ts":"2022-04-16T15:55:31.643Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":51387,"took":"539.666¬µs"}

* 
* ==> kernel <==
*  15:56:36 up  1:20,  0 users,  load average: 0.25, 0.45, 0.65
Linux minikube 5.13.0-39-generic #44~20.04.1-Ubuntu SMP Thu Mar 24 16:43:35 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.2 LTS"

* 
* ==> kube-apiserver [3e74d0505e44] <==
* W0415 20:37:38.934201       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:38.937770       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:38.954238       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:38.972040       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:38.988241       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:39.048113       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:39.063630       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:39.065045       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:39.099622       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:39.137482       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:39.137932       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:39.259011       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:39.329438       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:41.510370       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:41.522809       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:41.605929       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:41.623433       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:41.747145       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:41.824771       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:41.831408       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:41.897580       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:41.899042       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:41.960709       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.016861       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.101956       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.138471       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.141058       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.143593       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.163536       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.192007       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.237503       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.262388       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.334802       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.362606       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.372067       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.380516       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.404512       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.410996       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.429888       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.465724       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.491700       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.516004       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.522416       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.542425       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.621845       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.635326       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.639045       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.659419       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.718718       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.722163       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.746188       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.784186       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.811533       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.853627       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.920093       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.972787       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:42.979327       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:43.099068       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:43.127013       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0415 20:37:43.205826       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...

* 
* ==> kube-apiserver [6bb0ccdf221a] <==
* W0416 14:40:32.321686       1 genericapiserver.go:538] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
I0416 14:40:32.325785       1 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0416 14:40:32.325801       1 plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.
W0416 14:40:32.366435       1 genericapiserver.go:538] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0416 14:40:33.250220       1 dynamic_cafile_content.go:156] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0416 14:40:33.250220       1 dynamic_cafile_content.go:156] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0416 14:40:33.250549       1 secure_serving.go:266] Serving securely on [::]:8443
I0416 14:40:33.250603       1 dynamic_serving_content.go:131] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0416 14:40:33.251839       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0416 14:40:33.251852       1 shared_informer.go:240] Waiting for caches to sync for cluster_authentication_trust_controller
I0416 14:40:33.251875       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0416 14:40:33.261872       1 customresource_discovery_controller.go:209] Starting DiscoveryController
I0416 14:40:33.261911       1 dynamic_cafile_content.go:156] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0416 14:40:33.268461       1 dynamic_serving_content.go:131] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0416 14:40:33.268719       1 dynamic_cafile_content.go:156] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0416 14:40:33.268963       1 available_controller.go:491] Starting AvailableConditionController
I0416 14:40:33.269039       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0416 14:40:33.269923       1 autoregister_controller.go:141] Starting autoregister controller
I0416 14:40:33.269946       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0416 14:40:33.270129       1 controller.go:83] Starting OpenAPI AggregationController
I0416 14:40:33.270395       1 apf_controller.go:317] Starting API Priority and Fairness config controller
I0416 14:40:33.271027       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0416 14:40:33.279185       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0416 14:40:33.295799       1 controller.go:85] Starting OpenAPI controller
I0416 14:40:33.295963       1 naming_controller.go:291] Starting NamingConditionController
I0416 14:40:33.296057       1 establishing_controller.go:76] Starting EstablishingController
I0416 14:40:33.296764       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0416 14:40:33.296855       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0416 14:40:33.296940       1 crd_finalizer.go:266] Starting CRDFinalizer
I0416 14:40:33.306945       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0416 14:40:33.306964       1 shared_informer.go:240] Waiting for caches to sync for crd-autoregister
I0416 14:40:33.306977       1 shared_informer.go:247] Caches are synced for crd-autoregister 
I0416 14:40:33.362685       1 shared_informer.go:247] Caches are synced for cluster_authentication_trust_controller 
E0416 14:40:33.367534       1 controller.go:157] Error removing old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I0416 14:40:33.372751       1 cache.go:39] Caches are synced for autoregister controller
I0416 14:40:33.387756       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0416 14:40:33.469240       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0416 14:40:33.479205       1 apf_controller.go:322] Running API Priority and Fairness config worker
I0416 14:40:33.484022       1 shared_informer.go:247] Caches are synced for node_authorizer 
I0416 14:40:33.503961       1 controller.go:611] quota admission added evaluator for: leases.coordination.k8s.io
I0416 14:40:34.251078       1 controller.go:132] OpenAPI AggregationController: action for item : Nothing (removed from the queue).
I0416 14:40:34.251116       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0416 14:40:34.264959       1 storage_scheduling.go:109] all system priority classes are created successfully or already exist.
I0416 14:40:35.024798       1 controller.go:611] quota admission added evaluator for: serviceaccounts
I0416 14:40:35.036640       1 controller.go:611] quota admission added evaluator for: deployments.apps
I0416 14:40:35.073506       1 controller.go:611] quota admission added evaluator for: daemonsets.apps
I0416 14:40:35.087545       1 controller.go:611] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0416 14:40:35.095445       1 controller.go:611] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0416 14:40:37.951143       1 controller.go:611] quota admission added evaluator for: events.events.k8s.io
I0416 14:40:45.892217       1 controller.go:611] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0416 14:40:45.892217       1 controller.go:611] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0416 14:40:45.930156       1 controller.go:611] quota admission added evaluator for: endpoints
I0416 14:48:46.943512       1 controller.go:611] quota admission added evaluator for: namespaces
I0416 14:48:47.054867       1 alloc.go:329] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs=map[IPv4:10.101.208.24]
I0416 14:48:47.069435       1 alloc.go:329] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs=map[IPv4:10.101.47.93]
I0416 14:48:47.080343       1 controller.go:611] quota admission added evaluator for: replicasets.apps
I0416 14:48:47.087486       1 controller.go:611] quota admission added evaluator for: jobs.batch
W0416 15:02:26.910829       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0416 15:26:25.120460       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0416 15:56:28.430064       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted

* 
* ==> kube-controller-manager [1f581085ed20] <==
* I0415 19:21:11.651942       1 event.go:294] "Event occurred" object="default/query-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set query-depl-74f94ddc5 to 0"
I0415 19:21:11.658246       1 event.go:294] "Event occurred" object="default/query-depl-74f94ddc5" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: query-depl-74f94ddc5-9cv7k"
I0415 19:31:38.887012       1 event.go:294] "Event occurred" object="default/posts-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set posts-depl-7498cf4f64 to 1"
I0415 19:31:38.893459       1 event.go:294] "Event occurred" object="default/posts-depl-7498cf4f64" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: posts-depl-7498cf4f64-7v5kq"
I0415 19:31:46.890598       1 event.go:294] "Event occurred" object="default/posts-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set posts-depl-67777c9958 to 0"
I0415 19:31:46.899062       1 event.go:294] "Event occurred" object="default/posts-depl-67777c9958" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: posts-depl-67777c9958-grv9t"
W0415 19:31:47.910603       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "default/posts-srv", retrying. Error: EndpointSlice informer cache is out of date
I0415 19:35:02.372823       1 event.go:294] "Event occurred" object="default/posts-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set posts-depl-86cbb45f5f to 1"
I0415 19:35:02.378007       1 event.go:294] "Event occurred" object="default/posts-depl-86cbb45f5f" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: posts-depl-86cbb45f5f-9qdtb"
I0415 19:35:10.244459       1 event.go:294] "Event occurred" object="default/posts-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set posts-depl-7498cf4f64 to 0"
I0415 19:35:10.248782       1 event.go:294] "Event occurred" object="default/posts-depl-7498cf4f64" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: posts-depl-7498cf4f64-7v5kq"
I0415 19:38:34.721671       1 event.go:294] "Event occurred" object="default/posts-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set posts-depl-7cb7968d6c to 1"
I0415 19:38:34.731434       1 event.go:294] "Event occurred" object="default/posts-depl-7cb7968d6c" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: posts-depl-7cb7968d6c-w2m9c"
I0415 19:38:42.716116       1 event.go:294] "Event occurred" object="default/posts-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set posts-depl-86cbb45f5f to 0"
I0415 19:38:42.725105       1 event.go:294] "Event occurred" object="default/posts-depl-86cbb45f5f" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: posts-depl-86cbb45f5f-9qdtb"
W0415 19:38:43.736833       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "default/posts-srv", retrying. Error: EndpointSlice informer cache is out of date
I0415 19:43:25.851668       1 event.go:294] "Event occurred" object="default/posts-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set posts-depl-68c7f575bf to 1"
I0415 19:43:25.859318       1 event.go:294] "Event occurred" object="default/posts-depl-68c7f575bf" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: posts-depl-68c7f575bf-nzd6d"
I0415 19:43:33.037082       1 event.go:294] "Event occurred" object="default/posts-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set posts-depl-7cb7968d6c to 0"
I0415 19:43:33.045961       1 event.go:294] "Event occurred" object="default/posts-depl-7cb7968d6c" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: posts-depl-7cb7968d6c-w2m9c"
I0415 19:44:54.064047       1 event.go:294] "Event occurred" object="default/posts-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set posts-depl-55d7489c44 to 1"
I0415 19:44:54.072230       1 event.go:294] "Event occurred" object="default/posts-depl-55d7489c44" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: posts-depl-55d7489c44-sfk9w"
I0415 19:44:59.112099       1 event.go:294] "Event occurred" object="default/posts-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set posts-depl-68c7f575bf to 0"
I0415 19:44:59.120853       1 event.go:294] "Event occurred" object="default/posts-depl-68c7f575bf" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: posts-depl-68c7f575bf-nzd6d"
I0415 19:51:20.539356       1 event.go:294] "Event occurred" object="default/comments-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set comments-depl-7f5ccd866d to 1"
I0415 19:51:20.550571       1 event.go:294] "Event occurred" object="default/comments-depl-7f5ccd866d" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: comments-depl-7f5ccd866d-xrdk7"
I0415 19:51:25.586846       1 event.go:294] "Event occurred" object="default/comments-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set comments-depl-64ddd95c5c to 0"
I0415 19:51:25.597731       1 event.go:294] "Event occurred" object="default/comments-depl-64ddd95c5c" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: comments-depl-64ddd95c5c-cvkkm"
I0415 19:51:33.804921       1 event.go:294] "Event occurred" object="default/event-bus-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set event-bus-depl-7d66d4b65 to 1"
I0415 19:51:33.807705       1 event.go:294] "Event occurred" object="default/event-bus-depl-7d66d4b65" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: event-bus-depl-7d66d4b65-skzj2"
I0415 19:51:38.765065       1 event.go:294] "Event occurred" object="default/event-bus-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set event-bus-depl-747bfc7fcc to 0"
I0415 19:51:38.773366       1 event.go:294] "Event occurred" object="default/event-bus-depl-747bfc7fcc" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: event-bus-depl-747bfc7fcc-wzgw6"
I0415 19:51:43.274737       1 event.go:294] "Event occurred" object="default/moderation-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set moderation-depl-75947c4dc5 to 1"
I0415 19:51:43.283938       1 event.go:294] "Event occurred" object="default/moderation-depl-75947c4dc5" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: moderation-depl-75947c4dc5-rm92g"
I0415 19:51:48.056792       1 event.go:294] "Event occurred" object="default/moderation-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set moderation-depl-d85d7bfb to 0"
I0415 19:51:48.065017       1 event.go:294] "Event occurred" object="default/moderation-depl-d85d7bfb" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: moderation-depl-d85d7bfb-j6mpf"
I0415 19:51:52.623579       1 event.go:294] "Event occurred" object="default/posts-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set posts-depl-78c8ccdff8 to 1"
I0415 19:51:52.631972       1 event.go:294] "Event occurred" object="default/posts-depl-78c8ccdff8" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: posts-depl-78c8ccdff8-fwlk6"
I0415 19:51:57.379670       1 event.go:294] "Event occurred" object="default/posts-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set posts-depl-55d7489c44 to 0"
I0415 19:51:57.388773       1 event.go:294] "Event occurred" object="default/posts-depl-55d7489c44" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: posts-depl-55d7489c44-sfk9w"
I0415 19:52:00.763784       1 event.go:294] "Event occurred" object="default/query-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set query-depl-848d467ff9 to 1"
I0415 19:52:00.771641       1 event.go:294] "Event occurred" object="default/query-depl-848d467ff9" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: query-depl-848d467ff9-p4bbl"
I0415 19:52:05.552880       1 event.go:294] "Event occurred" object="default/query-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set query-depl-748cbbdb4c to 0"
I0415 19:52:05.562643       1 event.go:294] "Event occurred" object="default/query-depl-748cbbdb4c" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: query-depl-748cbbdb4c-mgjfh"
I0415 19:55:14.104389       1 event.go:294] "Event occurred" object="default/query-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set query-depl-579bbc8f69 to 1"
I0415 19:55:14.111650       1 event.go:294] "Event occurred" object="default/query-depl-579bbc8f69" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: query-depl-579bbc8f69-btbcv"
I0415 19:55:21.936421       1 event.go:294] "Event occurred" object="default/query-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set query-depl-848d467ff9 to 0"
I0415 19:55:21.940824       1 event.go:294] "Event occurred" object="default/query-depl-848d467ff9" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: query-depl-848d467ff9-p4bbl"
I0415 19:58:20.789954       1 event.go:294] "Event occurred" object="default/query-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set query-depl-7df9665f99 to 1"
I0415 19:58:20.805613       1 event.go:294] "Event occurred" object="default/query-depl-7df9665f99" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: query-depl-7df9665f99-jqr88"
I0415 19:58:31.132901       1 event.go:294] "Event occurred" object="default/query-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set query-depl-579bbc8f69 to 0"
I0415 19:58:31.137909       1 event.go:294] "Event occurred" object="default/query-depl-579bbc8f69" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: query-depl-579bbc8f69-btbcv"
I0415 20:03:06.448770       1 event.go:294] "Event occurred" object="default/query-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set query-depl-86b67cf65f to 1"
I0415 20:03:06.457490       1 event.go:294] "Event occurred" object="default/query-depl-86b67cf65f" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: query-depl-86b67cf65f-k64fr"
I0415 20:03:16.380085       1 event.go:294] "Event occurred" object="default/query-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set query-depl-7df9665f99 to 0"
I0415 20:03:16.386154       1 event.go:294] "Event occurred" object="default/query-depl-7df9665f99" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: query-depl-7df9665f99-jqr88"
I0415 20:08:20.776440       1 event.go:294] "Event occurred" object="default/event-bus-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set event-bus-depl-64dcd86bc5 to 1"
I0415 20:08:20.784396       1 event.go:294] "Event occurred" object="default/event-bus-depl-64dcd86bc5" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: event-bus-depl-64dcd86bc5-zfz2w"
I0415 20:08:32.138674       1 event.go:294] "Event occurred" object="default/event-bus-depl" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set event-bus-depl-7d66d4b65 to 0"
I0415 20:08:32.147316       1 event.go:294] "Event occurred" object="default/event-bus-depl-7d66d4b65" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: event-bus-depl-7d66d4b65-skzj2"

* 
* ==> kube-controller-manager [239dee87cc70] <==
* I0416 14:40:46.596764       1 shared_informer.go:247] Caches are synced for garbage collector 
I0416 14:40:46.596781       1 garbagecollector.go:155] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I0416 14:48:47.082436       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-5b6f946f99 to 1"
I0416 14:48:47.090383       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0416 14:48:47.095077       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-5b6f946f99" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-5b6f946f99-b7dcq"
I0416 14:48:47.115189       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0416 14:48:47.127003       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0416 14:48:47.127081       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0416 14:48:47.127435       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-patch-xscqc"
I0416 14:48:47.127670       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-create-kgk2v"
I0416 14:48:47.171015       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0416 14:48:47.171234       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0416 14:48:47.171632       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0416 14:48:47.173658       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-patch
E0416 14:48:47.183452       1 job_controller.go:488] syncing job: tracking status: adding uncounted pods to status: Operation cannot be fulfilled on jobs.batch "ingress-nginx-admission-patch": the object has been modified; please apply your changes to the latest version and try again
I0416 14:48:47.190884       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0416 14:48:47.204086       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0416 14:48:56.426442       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0416 14:48:56.442558       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0416 14:48:56.445857       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0416 14:48:56.450491       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0416 14:48:56.450555       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0416 14:48:56.456751       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0416 14:48:57.458239       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0416 14:48:57.463606       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0416 14:48:57.468057       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0416 14:48:57.468204       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I0416 14:48:57.472965       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0416 14:48:58.476964       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0416 15:11:36.823894       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Service" apiVersion="v1" type="Normal" reason="Type" message="LoadBalancer -> NodePort"
I0416 15:11:36.896362       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-cc8496874 to 1"
I0416 15:11:36.962057       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ingress-nginx-controller-5b6f946f99 to 0"
I0416 15:11:36.962371       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-cc8496874" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-cc8496874-jhsjq"
I0416 15:11:37.001880       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-5b6f946f99" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ingress-nginx-controller-5b6f946f99-b7dcq"
W0416 15:11:48.823873       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "ingress-nginx/ingress-nginx-controller", retrying. Error: EndpointSlice informer cache is out of date
I0416 15:27:47.846968       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Service" apiVersion="v1" type="Normal" reason="Type" message="NodePort -> LoadBalancer"
I0416 15:27:47.906160       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-5b6f946f99 to 1"
I0416 15:27:47.923011       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-5b6f946f99" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-5b6f946f99-qgz5p"
I0416 15:28:08.002468       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ingress-nginx-controller-cc8496874 to 0"
I0416 15:28:08.020267       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-cc8496874" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ingress-nginx-controller-cc8496874-jhsjq"
I0416 15:29:15.712469       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Service" apiVersion="v1" type="Normal" reason="Type" message="LoadBalancer -> NodePort"
I0416 15:29:15.803970       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-cc8496874 to 1"
I0416 15:29:15.866160       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ingress-nginx-controller-5b6f946f99 to 0"
I0416 15:29:15.887514       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-5b6f946f99" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ingress-nginx-controller-5b6f946f99-qgz5p"
I0416 15:29:15.896006       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-cc8496874" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-cc8496874-6q755"
W0416 15:29:15.956852       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "ingress-nginx/ingress-nginx-controller-admission", retrying. Error: EndpointSlice informer cache is out of date
I0416 15:31:01.321021       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-7fc8d55869 to 1"
I0416 15:31:01.333660       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-7fc8d55869" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-7fc8d55869-5fw8k"
I0416 15:31:51.434429       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ingress-nginx-controller-cc8496874 to 0"
I0416 15:31:51.543032       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-cc8496874" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ingress-nginx-controller-cc8496874-6q755"
I0416 15:36:46.851194       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Service" apiVersion="v1" type="Normal" reason="Type" message="NodePort -> LoadBalancer"
I0416 15:36:46.896271       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-5b6f946f99 to 1"
I0416 15:36:46.910253       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-5b6f946f99" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-5b6f946f99-hgf45"
I0416 15:37:06.965706       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ingress-nginx-controller-7fc8d55869 to 0"
I0416 15:37:06.974066       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-7fc8d55869" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ingress-nginx-controller-7fc8d55869-5fw8k"
I0416 15:41:28.633455       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Service" apiVersion="v1" type="Normal" reason="Type" message="LoadBalancer -> NodePort"
I0416 15:41:28.710653       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-cc8496874 to 1"
I0416 15:41:28.770238       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-cc8496874" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-cc8496874-gqrww"
I0416 15:41:28.775423       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ingress-nginx-controller-5b6f946f99 to 0"
I0416 15:41:28.800469       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-5b6f946f99" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ingress-nginx-controller-5b6f946f99-hgf45"

* 
* ==> kube-proxy [786d4b281bf7] <==
* I0416 14:40:37.897912       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I0416 14:40:37.897961       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I0416 14:40:37.897990       1 server_others.go:561] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I0416 14:40:37.942054       1 server_others.go:206] "Using iptables Proxier"
I0416 14:40:37.942084       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0416 14:40:37.942092       1 server_others.go:214] "Creating dualStackProxier for iptables"
I0416 14:40:37.942114       1 server_others.go:491] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I0416 14:40:37.943483       1 server.go:656] "Version info" version="v1.23.3"
I0416 14:40:37.945798       1 config.go:317] "Starting service config controller"
I0416 14:40:37.945830       1 config.go:226] "Starting endpoint slice config controller"
I0416 14:40:37.947184       1 shared_informer.go:240] Waiting for caches to sync for service config
I0416 14:40:37.947185       1 shared_informer.go:240] Waiting for caches to sync for endpoint slice config
I0416 14:40:38.047474       1 shared_informer.go:247] Caches are synced for endpoint slice config 
I0416 14:40:38.047474       1 shared_informer.go:247] Caches are synced for service config 
E0416 14:40:38.087587       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :31934: bind: address already in use" port={Description:nodePort for default/posts-srv:posts IP: IPFamily:4 Port:31934 Protocol:TCP}
E0416 14:48:47.092100       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :31691: bind: address already in use" port={Description:nodePort for ingress-nginx/ingress-nginx-controller:http IP: IPFamily:4 Port:31691 Protocol:TCP}
E0416 14:48:47.092346       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :31510: bind: address already in use" port={Description:nodePort for ingress-nginx/ingress-nginx-controller:https IP: IPFamily:4 Port:31510 Protocol:TCP}
E0416 15:11:37.106378       1 service_health.go:187] "Healthcheck closed" err="accept tcp [::]:31170: use of closed network connection" service="ingress-nginx/ingress-nginx-controller"
E0416 15:29:15.975880       1 service_health.go:187] "Healthcheck closed" err="accept tcp [::]:32521: use of closed network connection" service="ingress-nginx/ingress-nginx-controller"
E0416 15:41:28.894956       1 service_health.go:187] "Healthcheck closed" err="accept tcp [::]:32107: use of closed network connection" service="ingress-nginx/ingress-nginx-controller"

* 
* ==> kube-proxy [9cfd7ba83291] <==
* I0415 09:06:02.300641       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I0415 09:06:02.300699       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I0415 09:06:02.300738       1 server_others.go:561] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I0415 09:06:02.439462       1 server_others.go:206] "Using iptables Proxier"
I0415 09:06:02.439486       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0415 09:06:02.439494       1 server_others.go:214] "Creating dualStackProxier for iptables"
I0415 09:06:02.439515       1 server_others.go:491] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I0415 09:06:02.441628       1 server.go:656] "Version info" version="v1.23.3"
I0415 09:06:02.476305       1 config.go:317] "Starting service config controller"
I0415 09:06:02.476319       1 shared_informer.go:240] Waiting for caches to sync for service config
I0415 09:06:02.476380       1 config.go:226] "Starting endpoint slice config controller"
I0415 09:06:02.476385       1 shared_informer.go:240] Waiting for caches to sync for endpoint slice config
I0415 09:06:02.578221       1 shared_informer.go:247] Caches are synced for service config 
I0415 09:06:02.578295       1 shared_informer.go:247] Caches are synced for endpoint slice config 
E0415 09:06:02.793866       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :31934: bind: address already in use" port={Description:nodePort for default/posts-srv:posts IP: IPFamily:4 Port:31934 Protocol:TCP}

* 
* ==> kube-scheduler [8ee3b422b61e] <==
* E0416 14:40:30.879551       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0416 14:40:30.879605       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0416 14:40:30.879626       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0416 14:40:30.879685       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0416 14:40:30.879705       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0416 14:40:30.879763       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0416 14:40:30.879784       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0416 14:40:30.880548       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0416 14:40:30.880585       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0416 14:40:30.880779       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0416 14:40:30.880821       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0416 14:40:30.881718       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0416 14:40:30.881765       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0416 14:40:30.879840       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0416 14:40:30.882142       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0416 14:40:30.882087       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1beta1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1beta1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0416 14:40:30.882246       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1beta1.CSIStorageCapacity: failed to list *v1beta1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1beta1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0416 14:40:33.295027       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0416 14:40:33.295140       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0416 14:40:33.295293       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0416 14:40:33.295339       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0416 14:40:33.295537       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0416 14:40:33.295588       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0416 14:40:33.295714       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0416 14:40:33.295766       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0416 14:40:33.295951       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0416 14:40:33.296047       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0416 14:40:33.296233       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0416 14:40:33.296247       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0416 14:40:33.296290       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0416 14:40:33.296328       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0416 14:40:33.296329       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0416 14:40:33.296347       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0416 14:40:33.296299       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0416 14:40:33.296357       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0416 14:40:33.296367       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0416 14:40:33.296413       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0416 14:40:33.296414       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1beta1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0416 14:40:33.296478       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1beta1.CSIStorageCapacity: failed to list *v1beta1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0416 14:40:33.296454       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0416 14:40:33.296552       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0416 14:40:33.296635       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0416 14:40:33.296672       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0416 14:40:33.296980       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0416 14:40:33.296999       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0416 14:40:33.323622       1 reflector.go:324] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:205: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0416 14:40:33.323661       1 reflector.go:138] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:205: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0416 14:40:35.500271       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"default\" not found" namespace="default"
E0416 14:40:35.500306       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"default\" not found" namespace="default"
E0416 14:40:35.500320       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"default\" not found" namespace="default"
E0416 14:40:35.500330       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kube-system\" not found" namespace="kube-system"
E0416 14:40:35.500343       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kube-system\" not found" namespace="kube-system"
E0416 14:40:35.500363       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kube-system\" not found" namespace="kube-system"
E0416 14:40:35.500375       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"default\" not found" namespace="default"
E0416 14:40:35.500382       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"default\" not found" namespace="default"
E0416 14:40:35.500396       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kube-system\" not found" namespace="kube-system"
E0416 14:40:35.500404       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kube-system\" not found" namespace="kube-system"
E0416 14:40:35.500423       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kube-system\" not found" namespace="kube-system"
E0416 14:40:35.500437       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kube-system\" not found" namespace="kube-system"
I0416 14:40:36.375552       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 

* 
* ==> kube-scheduler [a3d411c9ef50] <==
* I0415 09:05:57.988222       1 serving.go:348] Generated self-signed cert in-memory
W0415 09:06:00.586281       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0415 09:06:00.586356       1 authentication.go:345] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0415 09:06:00.586388       1 authentication.go:346] Continuing without authentication configuration. This may treat all requests as anonymous.
W0415 09:06:00.586438       1 authentication.go:347] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0415 09:06:00.603556       1 server.go:139] "Starting Kubernetes Scheduler" version="v1.23.3"
I0415 09:06:00.608637       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0415 09:06:00.608660       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0415 09:06:00.608978       1 secure_serving.go:200] Serving securely on 127.0.0.1:10259
I0415 09:06:00.609069       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0415 09:06:00.709443       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 
I0415 20:37:33.378604       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0415 20:37:33.389332       1 secure_serving.go:311] Stopped listening on 127.0.0.1:10259
I0415 20:37:33.389379       1 configmap_cafile_content.go:222] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"

* 
* ==> kubelet <==
* -- Logs begin at Sat 2022-04-16 14:40:16 UTC, end at Sat 2022-04-16 15:56:36 UTC. --
Apr 16 15:31:01 minikube kubelet[819]: I0416 15:31:01.367678     819 topology_manager.go:200] "Topology Admit Handler"
Apr 16 15:31:01 minikube kubelet[819]: I0416 15:31:01.459157     819 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/2ef56eb0-ee7f-4655-84de-67ea92d3db1a-webhook-cert\") pod \"ingress-nginx-controller-7fc8d55869-5fw8k\" (UID: \"2ef56eb0-ee7f-4655-84de-67ea92d3db1a\") " pod="ingress-nginx/ingress-nginx-controller-7fc8d55869-5fw8k"
Apr 16 15:31:01 minikube kubelet[819]: I0416 15:31:01.459242     819 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-v7qcq\" (UniqueName: \"kubernetes.io/projected/2ef56eb0-ee7f-4655-84de-67ea92d3db1a-kube-api-access-v7qcq\") pod \"ingress-nginx-controller-7fc8d55869-5fw8k\" (UID: \"2ef56eb0-ee7f-4655-84de-67ea92d3db1a\") " pod="ingress-nginx/ingress-nginx-controller-7fc8d55869-5fw8k"
Apr 16 15:31:02 minikube kubelet[819]: I0416 15:31:02.034595     819 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ingress-nginx/ingress-nginx-controller-7fc8d55869-5fw8k through plugin: invalid network status for"
Apr 16 15:31:02 minikube kubelet[819]: I0416 15:31:02.213677     819 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ingress-nginx/ingress-nginx-controller-7fc8d55869-5fw8k through plugin: invalid network status for"
Apr 16 15:31:32 minikube kubelet[819]: I0416 15:31:32.657029     819 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ingress-nginx/ingress-nginx-controller-7fc8d55869-5fw8k through plugin: invalid network status for"
Apr 16 15:31:53 minikube kubelet[819]: I0416 15:31:53.231728     819 reconciler.go:192] "operationExecutor.UnmountVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/77bfbb52-9fec-4778-b36c-798d9054ec6a-webhook-cert\") pod \"77bfbb52-9fec-4778-b36c-798d9054ec6a\" (UID: \"77bfbb52-9fec-4778-b36c-798d9054ec6a\") "
Apr 16 15:31:53 minikube kubelet[819]: I0416 15:31:53.231807     819 reconciler.go:192] "operationExecutor.UnmountVolume started for volume \"kube-api-access-xprkv\" (UniqueName: \"kubernetes.io/projected/77bfbb52-9fec-4778-b36c-798d9054ec6a-kube-api-access-xprkv\") pod \"77bfbb52-9fec-4778-b36c-798d9054ec6a\" (UID: \"77bfbb52-9fec-4778-b36c-798d9054ec6a\") "
Apr 16 15:31:53 minikube kubelet[819]: I0416 15:31:53.235214     819 operation_generator.go:910] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/77bfbb52-9fec-4778-b36c-798d9054ec6a-kube-api-access-xprkv" (OuterVolumeSpecName: "kube-api-access-xprkv") pod "77bfbb52-9fec-4778-b36c-798d9054ec6a" (UID: "77bfbb52-9fec-4778-b36c-798d9054ec6a"). InnerVolumeSpecName "kube-api-access-xprkv". PluginName "kubernetes.io/projected", VolumeGidValue ""
Apr 16 15:31:53 minikube kubelet[819]: I0416 15:31:53.235487     819 operation_generator.go:910] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/77bfbb52-9fec-4778-b36c-798d9054ec6a-webhook-cert" (OuterVolumeSpecName: "webhook-cert") pod "77bfbb52-9fec-4778-b36c-798d9054ec6a" (UID: "77bfbb52-9fec-4778-b36c-798d9054ec6a"). InnerVolumeSpecName "webhook-cert". PluginName "kubernetes.io/secret", VolumeGidValue ""
Apr 16 15:31:53 minikube kubelet[819]: I0416 15:31:53.332943     819 reconciler.go:300] "Volume detached for volume \"kube-api-access-xprkv\" (UniqueName: \"kubernetes.io/projected/77bfbb52-9fec-4778-b36c-798d9054ec6a-kube-api-access-xprkv\") on node \"minikube\" DevicePath \"\""
Apr 16 15:31:53 minikube kubelet[819]: I0416 15:31:53.333016     819 reconciler.go:300] "Volume detached for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/77bfbb52-9fec-4778-b36c-798d9054ec6a-webhook-cert\") on node \"minikube\" DevicePath \"\""
Apr 16 15:31:53 minikube kubelet[819]: I0416 15:31:53.418730     819 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=77bfbb52-9fec-4778-b36c-798d9054ec6a path="/var/lib/kubelet/pods/77bfbb52-9fec-4778-b36c-798d9054ec6a/volumes"
Apr 16 15:31:53 minikube kubelet[819]: I0416 15:31:53.419054     819 kubelet_getters.go:300] "Path does not exist" path="/var/lib/kubelet/pods/77bfbb52-9fec-4778-b36c-798d9054ec6a/volumes"
Apr 16 15:31:54 minikube kubelet[819]: I0416 15:31:54.143385     819 scope.go:110] "RemoveContainer" containerID="edbf21b2ed2c44da184478c59e4c7ca8ec149e760493bc35a9d817a821e6d3df"
Apr 16 15:32:27 minikube kubelet[819]: E0416 15:32:27.407278     819 fsHandler.go:119] failed to collect filesystem stats - rootDiskErr: could not stat "/var/lib/docker/overlay2/cbd8d618907299897ba96483bfa5fdb77cf5d5679e2981f65ddb554f17c7d9eb/diff" to get inode usage: stat /var/lib/docker/overlay2/cbd8d618907299897ba96483bfa5fdb77cf5d5679e2981f65ddb554f17c7d9eb/diff: no such file or directory, extraDiskErr: could not stat "/var/lib/docker/containers/edbf21b2ed2c44da184478c59e4c7ca8ec149e760493bc35a9d817a821e6d3df" to get inode usage: stat /var/lib/docker/containers/edbf21b2ed2c44da184478c59e4c7ca8ec149e760493bc35a9d817a821e6d3df: no such file or directory
Apr 16 15:36:46 minikube kubelet[819]: I0416 15:36:46.913679     819 topology_manager.go:200] "Topology Admit Handler"
Apr 16 15:36:47 minikube kubelet[819]: I0416 15:36:47.076327     819 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-hmtmr\" (UniqueName: \"kubernetes.io/projected/25aaae9d-347f-4ad1-87df-12e7ac83a4d2-kube-api-access-hmtmr\") pod \"ingress-nginx-controller-5b6f946f99-hgf45\" (UID: \"25aaae9d-347f-4ad1-87df-12e7ac83a4d2\") " pod="ingress-nginx/ingress-nginx-controller-5b6f946f99-hgf45"
Apr 16 15:36:47 minikube kubelet[819]: I0416 15:36:47.076502     819 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/25aaae9d-347f-4ad1-87df-12e7ac83a4d2-webhook-cert\") pod \"ingress-nginx-controller-5b6f946f99-hgf45\" (UID: \"25aaae9d-347f-4ad1-87df-12e7ac83a4d2\") " pod="ingress-nginx/ingress-nginx-controller-5b6f946f99-hgf45"
Apr 16 15:36:47 minikube kubelet[819]: I0416 15:36:47.608692     819 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ingress-nginx/ingress-nginx-controller-5b6f946f99-hgf45 through plugin: invalid network status for"
Apr 16 15:36:47 minikube kubelet[819]: I0416 15:36:47.608758     819 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="8e53b4c7398d93e1453c27492e333cd763734301b8093404765af71d748d19a6"
Apr 16 15:36:48 minikube kubelet[819]: I0416 15:36:48.626255     819 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ingress-nginx/ingress-nginx-controller-5b6f946f99-hgf45 through plugin: invalid network status for"
Apr 16 15:37:18 minikube kubelet[819]: I0416 15:37:18.594180     819 reconciler.go:192] "operationExecutor.UnmountVolume started for volume \"kube-api-access-v7qcq\" (UniqueName: \"kubernetes.io/projected/2ef56eb0-ee7f-4655-84de-67ea92d3db1a-kube-api-access-v7qcq\") pod \"2ef56eb0-ee7f-4655-84de-67ea92d3db1a\" (UID: \"2ef56eb0-ee7f-4655-84de-67ea92d3db1a\") "
Apr 16 15:37:18 minikube kubelet[819]: I0416 15:37:18.594298     819 reconciler.go:192] "operationExecutor.UnmountVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/2ef56eb0-ee7f-4655-84de-67ea92d3db1a-webhook-cert\") pod \"2ef56eb0-ee7f-4655-84de-67ea92d3db1a\" (UID: \"2ef56eb0-ee7f-4655-84de-67ea92d3db1a\") "
Apr 16 15:37:18 minikube kubelet[819]: I0416 15:37:18.597715     819 operation_generator.go:910] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/2ef56eb0-ee7f-4655-84de-67ea92d3db1a-kube-api-access-v7qcq" (OuterVolumeSpecName: "kube-api-access-v7qcq") pod "2ef56eb0-ee7f-4655-84de-67ea92d3db1a" (UID: "2ef56eb0-ee7f-4655-84de-67ea92d3db1a"). InnerVolumeSpecName "kube-api-access-v7qcq". PluginName "kubernetes.io/projected", VolumeGidValue ""
Apr 16 15:37:18 minikube kubelet[819]: I0416 15:37:18.597889     819 operation_generator.go:910] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/2ef56eb0-ee7f-4655-84de-67ea92d3db1a-webhook-cert" (OuterVolumeSpecName: "webhook-cert") pod "2ef56eb0-ee7f-4655-84de-67ea92d3db1a" (UID: "2ef56eb0-ee7f-4655-84de-67ea92d3db1a"). InnerVolumeSpecName "webhook-cert". PluginName "kubernetes.io/secret", VolumeGidValue ""
Apr 16 15:37:18 minikube kubelet[819]: I0416 15:37:18.695066     819 reconciler.go:300] "Volume detached for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/2ef56eb0-ee7f-4655-84de-67ea92d3db1a-webhook-cert\") on node \"minikube\" DevicePath \"\""
Apr 16 15:37:18 minikube kubelet[819]: I0416 15:37:18.695137     819 reconciler.go:300] "Volume detached for volume \"kube-api-access-v7qcq\" (UniqueName: \"kubernetes.io/projected/2ef56eb0-ee7f-4655-84de-67ea92d3db1a-kube-api-access-v7qcq\") on node \"minikube\" DevicePath \"\""
Apr 16 15:37:19 minikube kubelet[819]: I0416 15:37:19.097580     819 scope.go:110] "RemoveContainer" containerID="5051befa38754d2335f25a9aa20b42ce8ef36ddad5bda0b51e8e830a71badea7"
Apr 16 15:37:19 minikube kubelet[819]: I0416 15:37:19.114720     819 scope.go:110] "RemoveContainer" containerID="5051befa38754d2335f25a9aa20b42ce8ef36ddad5bda0b51e8e830a71badea7"
Apr 16 15:37:19 minikube kubelet[819]: E0416 15:37:19.115400     819 remote_runtime.go:572] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: 5051befa38754d2335f25a9aa20b42ce8ef36ddad5bda0b51e8e830a71badea7" containerID="5051befa38754d2335f25a9aa20b42ce8ef36ddad5bda0b51e8e830a71badea7"
Apr 16 15:37:19 minikube kubelet[819]: I0416 15:37:19.115437     819 pod_container_deletor.go:52] "DeleteContainer returned error" containerID={Type:docker ID:5051befa38754d2335f25a9aa20b42ce8ef36ddad5bda0b51e8e830a71badea7} err="failed to get container status \"5051befa38754d2335f25a9aa20b42ce8ef36ddad5bda0b51e8e830a71badea7\": rpc error: code = Unknown desc = Error: No such container: 5051befa38754d2335f25a9aa20b42ce8ef36ddad5bda0b51e8e830a71badea7"
Apr 16 15:37:19 minikube kubelet[819]: E0416 15:37:19.406493     819 remote_runtime.go:479] "StopContainer from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 5051befa38754d2335f25a9aa20b42ce8ef36ddad5bda0b51e8e830a71badea7" containerID="5051befa38754d2335f25a9aa20b42ce8ef36ddad5bda0b51e8e830a71badea7"
Apr 16 15:37:19 minikube kubelet[819]: E0416 15:37:19.406597     819 kuberuntime_container.go:719] "Container termination failed with gracePeriod" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 5051befa38754d2335f25a9aa20b42ce8ef36ddad5bda0b51e8e830a71badea7" pod="ingress-nginx/ingress-nginx-controller-7fc8d55869-5fw8k" podUID=2ef56eb0-ee7f-4655-84de-67ea92d3db1a containerName="controller" containerID="docker://5051befa38754d2335f25a9aa20b42ce8ef36ddad5bda0b51e8e830a71badea7" gracePeriod=1
Apr 16 15:37:19 minikube kubelet[819]: E0416 15:37:19.406665     819 kuberuntime_container.go:744] "Kill container failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 5051befa38754d2335f25a9aa20b42ce8ef36ddad5bda0b51e8e830a71badea7" pod="ingress-nginx/ingress-nginx-controller-7fc8d55869-5fw8k" podUID=2ef56eb0-ee7f-4655-84de-67ea92d3db1a containerName="controller" containerID={Type:docker ID:5051befa38754d2335f25a9aa20b42ce8ef36ddad5bda0b51e8e830a71badea7}
Apr 16 15:37:19 minikube kubelet[819]: E0416 15:37:19.408048     819 kubelet.go:1777] failed to "KillContainer" for "controller" with KillContainerError: "rpc error: code = Unknown desc = Error response from daemon: No such container: 5051befa38754d2335f25a9aa20b42ce8ef36ddad5bda0b51e8e830a71badea7"
Apr 16 15:37:19 minikube kubelet[819]: E0416 15:37:19.408085     819 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"KillContainer\" for \"controller\" with KillContainerError: \"rpc error: code = Unknown desc = Error response from daemon: No such container: 5051befa38754d2335f25a9aa20b42ce8ef36ddad5bda0b51e8e830a71badea7\"" pod="ingress-nginx/ingress-nginx-controller-7fc8d55869-5fw8k" podUID=2ef56eb0-ee7f-4655-84de-67ea92d3db1a
Apr 16 15:37:19 minikube kubelet[819]: I0416 15:37:19.411864     819 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=2ef56eb0-ee7f-4655-84de-67ea92d3db1a path="/var/lib/kubelet/pods/2ef56eb0-ee7f-4655-84de-67ea92d3db1a/volumes"
Apr 16 15:37:27 minikube kubelet[819]: E0416 15:37:27.414414     819 fsHandler.go:119] failed to collect filesystem stats - rootDiskErr: could not stat "/var/lib/docker/overlay2/805f88140cf0b2f9230f397d60a4987431d19523f2cbfc3ebb77515f43b70ffd/diff" to get inode usage: stat /var/lib/docker/overlay2/805f88140cf0b2f9230f397d60a4987431d19523f2cbfc3ebb77515f43b70ffd/diff: no such file or directory, extraDiskErr: could not stat "/var/lib/docker/containers/5051befa38754d2335f25a9aa20b42ce8ef36ddad5bda0b51e8e830a71badea7" to get inode usage: stat /var/lib/docker/containers/5051befa38754d2335f25a9aa20b42ce8ef36ddad5bda0b51e8e830a71badea7: no such file or directory
Apr 16 15:41:28 minikube kubelet[819]: I0416 15:41:28.775240     819 topology_manager.go:200] "Topology Admit Handler"
Apr 16 15:41:28 minikube kubelet[819]: I0416 15:41:28.857089     819 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-stbdp\" (UniqueName: \"kubernetes.io/projected/6baa01a0-5e43-444a-a586-78254e301349-kube-api-access-stbdp\") pod \"ingress-nginx-controller-cc8496874-gqrww\" (UID: \"6baa01a0-5e43-444a-a586-78254e301349\") " pod="ingress-nginx/ingress-nginx-controller-cc8496874-gqrww"
Apr 16 15:41:28 minikube kubelet[819]: I0416 15:41:28.857483     819 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/6baa01a0-5e43-444a-a586-78254e301349-webhook-cert\") pod \"ingress-nginx-controller-cc8496874-gqrww\" (UID: \"6baa01a0-5e43-444a-a586-78254e301349\") " pod="ingress-nginx/ingress-nginx-controller-cc8496874-gqrww"
Apr 16 15:41:29 minikube kubelet[819]: I0416 15:41:29.487777     819 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ingress-nginx/ingress-nginx-controller-cc8496874-gqrww through plugin: invalid network status for"
Apr 16 15:41:29 minikube kubelet[819]: I0416 15:41:29.795888     819 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ingress-nginx/ingress-nginx-controller-cc8496874-gqrww through plugin: invalid network status for"
Apr 16 15:41:40 minikube kubelet[819]: I0416 15:41:40.535361     819 reconciler.go:192] "operationExecutor.UnmountVolume started for volume \"kube-api-access-hmtmr\" (UniqueName: \"kubernetes.io/projected/25aaae9d-347f-4ad1-87df-12e7ac83a4d2-kube-api-access-hmtmr\") pod \"25aaae9d-347f-4ad1-87df-12e7ac83a4d2\" (UID: \"25aaae9d-347f-4ad1-87df-12e7ac83a4d2\") "
Apr 16 15:41:40 minikube kubelet[819]: I0416 15:41:40.535443     819 reconciler.go:192] "operationExecutor.UnmountVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/25aaae9d-347f-4ad1-87df-12e7ac83a4d2-webhook-cert\") pod \"25aaae9d-347f-4ad1-87df-12e7ac83a4d2\" (UID: \"25aaae9d-347f-4ad1-87df-12e7ac83a4d2\") "
Apr 16 15:41:40 minikube kubelet[819]: I0416 15:41:40.538246     819 operation_generator.go:910] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/25aaae9d-347f-4ad1-87df-12e7ac83a4d2-webhook-cert" (OuterVolumeSpecName: "webhook-cert") pod "25aaae9d-347f-4ad1-87df-12e7ac83a4d2" (UID: "25aaae9d-347f-4ad1-87df-12e7ac83a4d2"). InnerVolumeSpecName "webhook-cert". PluginName "kubernetes.io/secret", VolumeGidValue ""
Apr 16 15:41:40 minikube kubelet[819]: I0416 15:41:40.538416     819 operation_generator.go:910] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/25aaae9d-347f-4ad1-87df-12e7ac83a4d2-kube-api-access-hmtmr" (OuterVolumeSpecName: "kube-api-access-hmtmr") pod "25aaae9d-347f-4ad1-87df-12e7ac83a4d2" (UID: "25aaae9d-347f-4ad1-87df-12e7ac83a4d2"). InnerVolumeSpecName "kube-api-access-hmtmr". PluginName "kubernetes.io/projected", VolumeGidValue ""
Apr 16 15:41:40 minikube kubelet[819]: I0416 15:41:40.635667     819 reconciler.go:300] "Volume detached for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/25aaae9d-347f-4ad1-87df-12e7ac83a4d2-webhook-cert\") on node \"minikube\" DevicePath \"\""
Apr 16 15:41:40 minikube kubelet[819]: I0416 15:41:40.635699     819 reconciler.go:300] "Volume detached for volume \"kube-api-access-hmtmr\" (UniqueName: \"kubernetes.io/projected/25aaae9d-347f-4ad1-87df-12e7ac83a4d2-kube-api-access-hmtmr\") on node \"minikube\" DevicePath \"\""
Apr 16 15:41:40 minikube kubelet[819]: I0416 15:41:40.974697     819 scope.go:110] "RemoveContainer" containerID="71dcc78875173fd00b007e1a4a725510c9bc40262fe0d376818e86fec44571d5"
Apr 16 15:41:40 minikube kubelet[819]: I0416 15:41:40.992081     819 scope.go:110] "RemoveContainer" containerID="71dcc78875173fd00b007e1a4a725510c9bc40262fe0d376818e86fec44571d5"
Apr 16 15:41:40 minikube kubelet[819]: E0416 15:41:40.992588     819 remote_runtime.go:572] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: 71dcc78875173fd00b007e1a4a725510c9bc40262fe0d376818e86fec44571d5" containerID="71dcc78875173fd00b007e1a4a725510c9bc40262fe0d376818e86fec44571d5"
Apr 16 15:41:40 minikube kubelet[819]: I0416 15:41:40.992623     819 pod_container_deletor.go:52] "DeleteContainer returned error" containerID={Type:docker ID:71dcc78875173fd00b007e1a4a725510c9bc40262fe0d376818e86fec44571d5} err="failed to get container status \"71dcc78875173fd00b007e1a4a725510c9bc40262fe0d376818e86fec44571d5\": rpc error: code = Unknown desc = Error: No such container: 71dcc78875173fd00b007e1a4a725510c9bc40262fe0d376818e86fec44571d5"
Apr 16 15:41:41 minikube kubelet[819]: E0416 15:41:41.406175     819 remote_runtime.go:479] "StopContainer from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 71dcc78875173fd00b007e1a4a725510c9bc40262fe0d376818e86fec44571d5" containerID="71dcc78875173fd00b007e1a4a725510c9bc40262fe0d376818e86fec44571d5"
Apr 16 15:41:41 minikube kubelet[819]: E0416 15:41:41.406231     819 kuberuntime_container.go:719] "Container termination failed with gracePeriod" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 71dcc78875173fd00b007e1a4a725510c9bc40262fe0d376818e86fec44571d5" pod="ingress-nginx/ingress-nginx-controller-5b6f946f99-hgf45" podUID=25aaae9d-347f-4ad1-87df-12e7ac83a4d2 containerName="controller" containerID="docker://71dcc78875173fd00b007e1a4a725510c9bc40262fe0d376818e86fec44571d5" gracePeriod=1
Apr 16 15:41:41 minikube kubelet[819]: E0416 15:41:41.406257     819 kuberuntime_container.go:744] "Kill container failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 71dcc78875173fd00b007e1a4a725510c9bc40262fe0d376818e86fec44571d5" pod="ingress-nginx/ingress-nginx-controller-5b6f946f99-hgf45" podUID=25aaae9d-347f-4ad1-87df-12e7ac83a4d2 containerName="controller" containerID={Type:docker ID:71dcc78875173fd00b007e1a4a725510c9bc40262fe0d376818e86fec44571d5}
Apr 16 15:41:41 minikube kubelet[819]: E0416 15:41:41.407791     819 kubelet.go:1777] failed to "KillContainer" for "controller" with KillContainerError: "rpc error: code = Unknown desc = Error response from daemon: No such container: 71dcc78875173fd00b007e1a4a725510c9bc40262fe0d376818e86fec44571d5"
Apr 16 15:41:41 minikube kubelet[819]: E0416 15:41:41.407827     819 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"KillContainer\" for \"controller\" with KillContainerError: \"rpc error: code = Unknown desc = Error response from daemon: No such container: 71dcc78875173fd00b007e1a4a725510c9bc40262fe0d376818e86fec44571d5\"" pod="ingress-nginx/ingress-nginx-controller-5b6f946f99-hgf45" podUID=25aaae9d-347f-4ad1-87df-12e7ac83a4d2
Apr 16 15:41:41 minikube kubelet[819]: I0416 15:41:41.413586     819 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=25aaae9d-347f-4ad1-87df-12e7ac83a4d2 path="/var/lib/kubelet/pods/25aaae9d-347f-4ad1-87df-12e7ac83a4d2/volumes"

* 
* ==> storage-provisioner [0c4bc1e4b953] <==
* I0416 14:40:37.640438       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0416 14:41:07.662274       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

* 
* ==> storage-provisioner [bcaaac5df9a0] <==
* I0416 14:41:21.712844       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0416 14:41:21.747765       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0416 14:41:21.748892       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0416 14:41:39.220659       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0416 14:41:39.220802       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_ff381a6a-80c2-44fc-aeb3-cd041308456d!
I0416 14:41:39.220802       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"01451d02-fefb-46b0-9d57-bd356070f088", APIVersion:"v1", ResourceVersion:"47448", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_ff381a6a-80c2-44fc-aeb3-cd041308456d became leader
I0416 14:41:39.322775       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_ff381a6a-80c2-44fc-aeb3-cd041308456d!

